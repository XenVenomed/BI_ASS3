{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7485d37953bfc88",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aab7f3508a7563c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:52:57.674025Z",
     "start_time": "2026-01-18T17:52:30.852734Z"
    }
   },
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "#!pip install -r requirements.txt"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;33mJupyter detected\u001B[0m\u001B[1;33m...\u001B[0m\r\n",
      "\u001B[1;32m2\u001B[0m\u001B[1;32m channel Terms of Service accepted\u001B[0m\r\n",
      "Channels:\r\n",
      " - defaults\r\n",
      "Platform: osx-arm64\r\n",
      "Collecting package metadata (repodata.json): done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "    current version: 25.5.1\r\n",
      "    latest version: 25.11.1\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c defaults conda\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/anaconda3/envs/BI2025\r\n",
      "\r\n",
      "  added / updated specs:\r\n",
      "    - python=3.11\r\n",
      "\r\n",
      "\r\n",
      "The following NEW packages will be INSTALLED:\r\n",
      "\r\n",
      "  bzip2              pkgs/main/osx-arm64::bzip2-1.0.8-h80987f9_6 \r\n",
      "  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2025.12.2-hca03da5_0 \r\n",
      "  expat              pkgs/main/osx-arm64::expat-2.7.3-h50f4ffc_4 \r\n",
      "  libcxx             pkgs/main/osx-arm64::libcxx-20.1.8-hd7fd590_1 \r\n",
      "  libexpat           pkgs/main/osx-arm64::libexpat-2.7.3-h50f4ffc_4 \r\n",
      "  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_1 \r\n",
      "  libzlib            pkgs/main/osx-arm64::libzlib-1.3.1-h5f15de7_0 \r\n",
      "  ncurses            pkgs/main/osx-arm64::ncurses-6.5-hee39554_0 \r\n",
      "  openssl            pkgs/main/osx-arm64::openssl-3.0.18-h9b4081a_0 \r\n",
      "  pip                pkgs/main/noarch::pip-25.3-pyhc872135_0 \r\n",
      "  python             pkgs/main/osx-arm64::python-3.11.14-hf701271_0 \r\n",
      "  readline           pkgs/main/osx-arm64::readline-8.3-h0b18652_0 \r\n",
      "  setuptools         pkgs/main/osx-arm64::setuptools-80.9.0-py311hca03da5_0 \r\n",
      "  sqlite             pkgs/main/osx-arm64::sqlite-3.51.1-hab6afd1_0 \r\n",
      "  tk                 pkgs/main/osx-arm64::tk-8.6.15-hcd8a7d5_0 \r\n",
      "  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \r\n",
      "  wheel              pkgs/main/osx-arm64::wheel-0.45.1-py311hca03da5_0 \r\n",
      "  xz                 pkgs/main/osx-arm64::xz-5.6.4-h80987f9_1 \r\n",
      "  zlib               pkgs/main/osx-arm64::zlib-1.3.1-h5f15de7_0 \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages:\r\n",
      "\r\n",
      "Preparing transaction: done\r\n",
      "Verifying transaction: done\r\n",
      "Executing transaction: done\r\n",
      "#\r\n",
      "# To activate this environment, use\r\n",
      "#\r\n",
      "#     $ conda activate BI2025\r\n",
      "#\r\n",
      "# To deactivate an active environment, use\r\n",
      "#\r\n",
      "#     $ conda deactivate\r\n",
      "\r\n",
      "\r\n",
      "CondaError: Run 'conda init' before 'conda activate'\r\n",
      "\r\n",
      "Collecting starvers@ git+https://github.com/GreenfishK/starvers.git (from -r requirements.txt (line 9))\r\n",
      "  Cloning https://github.com/GreenfishK/starvers.git to /private/var/folders/th/94pp8n193gvdkb8kbjt0vp3h0000gn/T/pip-install-18jmewqj/starvers_15dca417c7b04ba98896497ec4a5ad43\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/GreenfishK/starvers.git /private/var/folders/th/94pp8n193gvdkb8kbjt0vp3h0000gn/T/pip-install-18jmewqj/starvers_15dca417c7b04ba98896497ec4a5ad43\r\n",
      "  Resolved https://github.com/GreenfishK/starvers.git to commit dcb55edbd8b0506ec4eecce602159b332b35bedb\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: pandas in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.3.3)\r\n",
      "Requirement already satisfied: numpy in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (2.3.5)\r\n",
      "Requirement already satisfied: matplotlib in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (3.10.8)\r\n",
      "Requirement already satisfied: seaborn in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.13.2)\r\n",
      "Requirement already satisfied: plotly in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (6.5.0)\r\n",
      "Requirement already satisfied: requests in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (2.32.5)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (1.8.0)\r\n",
      "Requirement already satisfied: ipykernel in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (7.1.0)\r\n",
      "Requirement already satisfied: pytest==7.1.3 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (7.1.3)\r\n",
      "Requirement already satisfied: rdflib==6.2.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (6.2.0)\r\n",
      "Requirement already satisfied: setuptools==65.4.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (65.4.0)\r\n",
      "Requirement already satisfied: SPARQLWrapper==2.0.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (2.0.0)\r\n",
      "Requirement already satisfied: tzlocal==4.2 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (4.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\r\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pytest==7.1.3->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (25.4.0)\r\n",
      "Requirement already satisfied: iniconfig in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pytest==7.1.3->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (2.3.0)\r\n",
      "Requirement already satisfied: packaging in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pytest==7.1.3->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (25.0)\r\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pytest==7.1.3->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (1.6.0)\r\n",
      "Requirement already satisfied: py>=1.8.2 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pytest==7.1.3->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (1.11.0)\r\n",
      "Requirement already satisfied: tomli>=1.0.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pytest==7.1.3->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (2.3.0)\r\n",
      "Requirement already satisfied: isodate in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from rdflib==6.2.0->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (0.7.2)\r\n",
      "Requirement already satisfied: pyparsing in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from rdflib==6.2.0->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (3.2.5)\r\n",
      "Requirement already satisfied: pytz-deprecation-shim in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from tzlocal==4.2->starvers@ git+https://github.com/GreenfishK/starvers.git->-r requirements.txt (line 9)) (0.1.0.post0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 3)) (4.61.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.9)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 3)) (12.0.0)\r\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from plotly->-r requirements.txt (line 5)) (2.13.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from requests->-r requirements.txt (line 6)) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from requests->-r requirements.txt (line 6)) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from requests->-r requirements.txt (line 6)) (2.6.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from requests->-r requirements.txt (line 6)) (2025.11.12)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.16.3)\r\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.5.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\r\n",
      "Requirement already satisfied: appnope>=0.1.2 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (0.1.4)\r\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (0.2.3)\r\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (1.8.18)\r\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (9.8.0)\r\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (8.7.0)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (5.9.1)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (0.2.1)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (1.6.0)\r\n",
      "Requirement already satisfied: psutil>=5.7 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (7.1.3)\r\n",
      "Requirement already satisfied: pyzmq>=25 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (27.1.0)\r\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (6.5.3)\r\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipykernel->-r requirements.txt (line 8)) (5.14.3)\r\n",
      "Requirement already satisfied: decorator>=4.3.2 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (5.2.1)\r\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (1.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.18.1 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.19.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (3.0.52)\r\n",
      "Requirement already satisfied: pygments>=2.11.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (2.19.2)\r\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.6.3)\r\n",
      "Requirement already satisfied: wcwidth in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.2.14)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.8.5)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 8)) (4.5.1)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.7.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (2.2.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (3.0.1)\r\n",
      "Requirement already satisfied: pure-eval in /Users/avelardoramirez/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.2.3)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "93d9c7dfafd14ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:56:35.804125Z",
     "start_time": "2026-01-18T17:56:35.626572Z"
    }
   },
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "9c70c484e6493a6d",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d49622fa528b72",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "id": "266167ea2266b2ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:56:39.152288Z",
     "start_time": "2026-01-18T17:56:39.148243Z"
    }
   },
   "source": [
    "executed_by ='stud-id_12435655'  # Replace the digits after \"id_\" with your own student ID"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "9cce5bdfc6741d8a",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "id": "e60e0f59ce2ce961",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:56:42.285082Z",
     "start_time": "2026-01-18T17:56:42.282274Z"
    }
   },
   "source": [
    "# group id for this project\n",
    "group_id = '74'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12435655'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_01556207'  # Replace the digits after \"id_\" with student B's student ID"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c2cbd2cc837b9742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:56:46.203142Z",
     "start_time": "2026-01-18T17:56:46.200731Z"
    }
   },
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "740e8674bd5c5b90",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "id": "9ce57e490f9d95a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:56:47.739413Z",
     "start_time": "2026-01-18T17:56:47.736139Z"
    }
   },
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "f4b34199ed09b2ed",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2baa0d3237cfbc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:56:49.991922Z",
     "start_time": "2026-01-18T17:56:49.987615Z"
    }
   },
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "429111afab544641",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391288e441feec8",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "id": "cf6adfab76cff5bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:56:54.444672Z",
     "start_time": "2026-01-18T17:56:54.441413Z"
    }
   },
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "f1f67238e9fa8817",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "id": "c45c24326cd75d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T17:57:02.550699Z",
     "start_time": "2026-01-18T17:56:56.315070Z"
    }
   },
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Avelardo\" .',\n",
    "f':{student_a} foaf:familyName \"Ramirez\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"12435655\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Agon\" .',\n",
    "f':{student_b} foaf:familyName \"Sylejmani\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"01556207\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 30\u001B[39m\n\u001B[32m     13\u001B[39m reigstration_triples_b = [\n\u001B[32m     14\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudent_b\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m rdf:type foaf:Person .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     15\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudent_b\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m rdf:type prov:Agent .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     21\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudent_b\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m <http://purl.obolibrary.org/obo/IAO_0000219> \u001B[39m\u001B[33m\"\u001B[39m\u001B[33m01556207\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m^^xsd:string .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     22\u001B[39m ]\n\u001B[32m     24\u001B[39m role_triples = [\n\u001B[32m     25\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode_writer_role\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m rdf:type prov:Role .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     26\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode_executor_role\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m rdf:type prov:Role .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     27\u001B[39m ]\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreigstration_triples_a\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m engine.insert(reigstration_triples_b, prefixes=prefixes)\n\u001B[32m     32\u001B[39m engine.insert(role_triples, prefixes=prefixes)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "c903baa89e3d9b66",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "44aab9d0b38e6d87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:08:05.338433Z",
     "start_time": "2026-01-18T16:08:05.335624Z"
    }
   },
   "source": [
    "# Directory for obesity dataset\n",
    "obesity_data_path = os.path.join(\"data\", \"datasets\", \"obesity\")\n",
    "os.makedirs(obesity_data_path, exist_ok=True)\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "fefdeca106f7a705",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "id": "29b7dc0b9b9d0696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:08:13.110799Z",
     "start_time": "2026-01-18T16:08:07.205191Z"
    }
   },
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m## Each Activity that follows is part of the Business Understanding Phase\u001B[39;00m\n\u001B[32m      3\u001B[39m business_understanding_phase_executor = [\n\u001B[32m      4\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:business_understanding_phase rdf:type prov:Activity .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      5\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:business_understanding_phase rdfs:label \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBusiness Understanding Phase\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m, \u001B[38;5;66;03m## Phase 1: Business Understanding\u001B[39;00m\n\u001B[32m      6\u001B[39m ]\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbusiness_understanding_phase_executor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "8cb45e05aeebc735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:08:23.160768Z",
     "start_time": "2026-01-18T16:08:17.326067Z"
    }
   },
   "source": [
    "#############################################\n",
    "# Documentation - Business Understanding\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "**Data Source:**\n",
    "The dataset contains 2,111 records from individuals in Mexico, Peru, and Colombia,\n",
    "collected to estimate obesity levels based on eating habits and physical condition.\n",
    "The data includes 17 attributes covering demographics (age, gender, height, weight),\n",
    "eating habits (high-calorie food consumption, vegetable consumption, number of meals,\n",
    "water intake, alcohol consumption), and physical activity patterns (exercise frequency,\n",
    "technology usage time, transportation mode).\n",
    "\n",
    "**Business Scenario:**\n",
    "A public health agency in Latin America aims to combat the rising obesity epidemic\n",
    "by implementing targeted intervention programs. The agency needs an automated system\n",
    "to classify individuals into obesity risk categories based on their lifestyle and\n",
    "physical characteristics. This classification will enable:\n",
    "1. Early identification of at-risk populations\n",
    "2. Personalized health recommendations\n",
    "3. Resource allocation for intervention programs\n",
    "4. Monitoring of public health trends over time\n",
    "\n",
    "The system will be deployed as a web-based screening tool accessible to healthcare\n",
    "providers and wellness centers across Mexico, Peru, and Colombia.\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "The primary business objectives are:\n",
    "\n",
    "1. **Reduce Obesity Prevalence:** Support public health initiatives aimed at\n",
    "   reducing obesity rates.\n",
    "\n",
    "2. **Enable Targeted Interventions:** Provide healthcare professionals with an\n",
    "   accurate classification tool that identifies specific obesity risk categories,\n",
    "   allowing for customized intervention strategies for each risk group.\n",
    "\n",
    "3. **Improve Resource Allocation:** Help health agencies allocate resources\n",
    "   efficiently by identifying geographic regions and demographic groups with\n",
    "   highest obesity risk.\n",
    "\n",
    "4. **Support Preventive Care:** Enable early detection of obesity risk before\n",
    "   severe health complications develop.\n",
    "\n",
    "5. **Provide Data-Driven Insights:** Generate actionable insights about the\n",
    "   relationship between lifestyle factors and obesity levels to inform public health policy.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "The success of this business initiative will be measured by:\n",
    "\n",
    "1. **Adoption Rate:** Achieve 70% adoption rate among targeted healthcare\n",
    "   facilities within the first year of deployment.\n",
    "\n",
    "2. **Intervention Effectiveness:** Demonstrate that individuals identified as\n",
    "   high-risk who receive targeted interventions show measurable improvement\n",
    "   (BMI reduction of at least 2 points).\n",
    "\n",
    "3. **Cost-Effectiveness:** Reduce overall healthcare costs related to obesity\n",
    "   complications by 15% over 3 years through early intervention.\n",
    "\n",
    "4. **User Satisfaction:** Achieve at least 80% satisfaction rating from\n",
    "   healthcare providers using the tool, measured through user surveys.\n",
    "\n",
    "5. **Coverage:** Successfully screen at least 50,000 individuals within the\n",
    "   first year across the three target countries.\n",
    "\n",
    "6. **Actionability:** Ensure that 90% of high-risk classifications result in\n",
    "   documented intervention actions by healthcare providers.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "The specific data mining goals are:\n",
    "\n",
    "1. **Multi-class Classification:** Build a robust classifier that accurately\n",
    "   predicts obesity levels across all 7 categories:\n",
    "   - Insufficient Weight\n",
    "   - Normal Weight\n",
    "   - Overweight Level I\n",
    "   - Overweight Level II\n",
    "   - Obesity Type I\n",
    "   - Obesity Type II\n",
    "   - Obesity Type III\n",
    "\n",
    "2. **Feature Importance Analysis:** Identify which eating habits and physical\n",
    "   activity factors are most predictive of obesity levels to guide intervention\n",
    "   design.\n",
    "\n",
    "3. **Balanced Performance:** Achieve strong performance across all obesity\n",
    "   categories, not just the majority classes, ensuring reliable predictions\n",
    "   for minority obesity types.\n",
    "\n",
    "4. **Generalization:** Develop a model that generalizes well across different\n",
    "   demographic groups (age ranges, genders) and geographic regions.\n",
    "\n",
    "5. **Interpretability:** Create a model whose predictions can be explained to\n",
    "   healthcare providers and patients, supporting trust and actionable insights.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "The technical success criteria for the machine learning model are:\n",
    "\n",
    "1. **Overall Accuracy:** Achieve at least 90% overall classification accuracy\n",
    "   on held-out test data.\n",
    "\n",
    "2. **Balanced Performance:**\n",
    "   - Macro-averaged F1-score  0.85\n",
    "   - Minimum per-class recall  0.75 for each obesity category\n",
    "   - Macro-averaged precision  0.85\n",
    "\n",
    "3. **Confusion Matrix Analysis:** No obesity category should be systematically\n",
    "   misclassified as another.\n",
    "\n",
    "4. **Generalization:** Performance on validation set should be within 5% of\n",
    "   training set performance (avoid overfitting).\n",
    "\n",
    "5. **Reproducibility:** All results must be reproducible with documented random\n",
    "   seeds and preprocessing steps.\n",
    "\n",
    "6. **Baseline Comparison:** Outperform a simple baseline (random classifier,\n",
    "   majority class classifier) by at least 60 percentage points.\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "Several AI risk aspects require consideration:\n",
    "\n",
    "1. **Health Data Privacy:**\n",
    "   - Risk: Exposure of sensitive health information (weight, eating habits)\n",
    "   - Mitigation: Ensure anonymization, secure data handling, GDPR compliance\n",
    "\n",
    "2. **Bias and Fairness:**\n",
    "   - Risk: Model may perform differently across genders, age groups, or\n",
    "     geographic regions, leading to unfair treatment\n",
    "   - Concern: 77% synthetic data may not accurately represent real-world\n",
    "     distributions\n",
    "   - Action: Evaluate model performance separately for different demographic\n",
    "     subgroups; monitor for systematic bias\n",
    "\n",
    "3. **Stigmatization:**\n",
    "   - Risk: Incorrect obesity classification could lead to stigmatization,\n",
    "     discrimination in insurance/employment\n",
    "   - Mitigation: Predictions should be treated as screening tools, not\n",
    "     definitive diagnoses; require human oversight\n",
    "\n",
    "4. **Over-reliance on Automation:**\n",
    "   - Risk: Healthcare providers may rely solely on model predictions without\n",
    "     clinical judgment\n",
    "   - Mitigation: System should support decision-making, not replace professional\n",
    "     medical assessment\n",
    "\n",
    "5. **Limited Generalizability:**\n",
    "   - Risk: Model trained on Latin American populations may not generalize to\n",
    "     other regions/cultures with different dietary patterns\n",
    "   - Action: Clearly document limitations; validate before deployment in new\n",
    "     regions\n",
    "\n",
    "6. **Synthetic Data Concerns:**\n",
    "   - Risk: 77% SMOTE-generated data may introduce artificial patterns not\n",
    "     present in real populations\n",
    "   - Action: Carefully evaluate model behavior; compare predictions on real vs.\n",
    "     synthetic data subsets\n",
    "\n",
    "7. **Feature Sensitivity:**\n",
    "   - Risk: Model may learn spurious correlations (e.g., gender stereotypes\n",
    "     about eating habits)\n",
    "   - Action: Analyze feature importance; test for protected attribute influence\n",
    "\n",
    "8. **Intervention Harm:**\n",
    "   - Risk: False positives could lead to unnecessary interventions; false\n",
    "     negatives could miss at-risk individuals\n",
    "   - Mitigation: Establish appropriate confidence thresholds; implement\n",
    "     human-in-the-loop verification for critical cases\n",
    "\"\"\"\n",
    "\n",
    "bu_ass_uuid_executor = \"bb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\"\n",
    "\n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .',\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# 1a\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 185\u001B[39m\n\u001B[32m    175\u001B[39m bu_ass_uuid_executor = \u001B[33m\"\u001B[39m\u001B[33mbb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    177\u001B[39m business_understanding_executor = [\n\u001B[32m    178\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:business_understanding rdf:type prov:Activity .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    179\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:business_understanding sc:isPartOf :business_understanding_phase .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    183\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbu_ass_uuid_executor\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m prov:hadRole :\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode_executor_role\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    184\u001B[39m ]\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbusiness_understanding_executor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    188\u001B[39m business_understanding_data_executor = [\n\u001B[32m    189\u001B[39m \u001B[38;5;66;03m# 1a\u001B[39;00m\n\u001B[32m    190\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:bu_data_source_and_scenario rdf:type prov:Entity .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    219\u001B[39m \n\u001B[32m    220\u001B[39m ]\n\u001B[32m    221\u001B[39m engine.insert(business_understanding_data_executor, prefixes=prefixes)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "7449a44b5e45affd",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513398373b11befe",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "id": "30708c603ed10eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:08:35.541947Z",
     "start_time": "2026-01-18T16:08:29.575732Z"
    }
   },
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "data_understanding_phase = [\n",
    "    f':data_understanding_phase rdf:type prov:Activity .',\n",
    "    f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .',\n",
    "]\n",
    "engine.insert(data_understanding_phase, prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m## Each Activity that follows is part of the Data Understanding Phase\u001B[39;00m\n\u001B[32m      3\u001B[39m data_understanding_phase = [\n\u001B[32m      4\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:data_understanding_phase rdf:type prov:Activity .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      5\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:data_understanding_phase rdfs:label \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mData Understanding Phase\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      6\u001B[39m ]\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_understanding_phase\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "146ffa9171e22a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:08:38.629890Z",
     "start_time": "2026-01-18T16:08:38.578825Z"
    }
   },
   "source": [
    "##############################################\n",
    "# Basic Information (2a) - Loading the data and Analyzing\n",
    "##############################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "obesity_data_path = os.path.join(\"data\", \"datasets\", \"obesity\")\n",
    "os.makedirs(obesity_data_path, exist_ok=True)\n",
    "\n",
    "# Capture start time\n",
    "start_time_load = now()\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(obesity_data_path, \"obesity_data.csv\"))\n",
    "\n",
    "# Capture end time\n",
    "end_time_load = now()\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Names:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nColumn Names and Types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nStatistical Summary:\\n{df.describe()}\")\n",
    "\n",
    "# Numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nNumeric features ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Data loading documentation will be included in comprehensive activity at end"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (2111, 17)\n",
      "\n",
      "Column Names:\n",
      "['Age', 'Gender', 'Height', 'Weight', 'CALC', 'FAVC', 'FCVC', 'NCP', 'SCC', 'SMOKE', 'CH2O', 'family_history_with_overweight', 'FAF', 'TUE', 'CAEC', 'MTRANS', 'NObeyesdad']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    Age  Gender  Height  Weight        CALC FAVC  FCVC  NCP  SCC SMOKE  CH2O  \\\n",
       "0  21.0  Female    1.62    64.0          no   no   2.0  3.0   no    no   2.0   \n",
       "1  21.0  Female    1.52    56.0   Sometimes   no   3.0  3.0  yes   yes   3.0   \n",
       "2  23.0    Male    1.80    77.0  Frequently   no   2.0  3.0   no    no   2.0   \n",
       "3  27.0    Male    1.80    87.0  Frequently   no   3.0  3.0   no    no   2.0   \n",
       "4  22.0    Male    1.78    89.8   Sometimes   no   2.0  1.0   no    no   2.0   \n",
       "\n",
       "  family_history_with_overweight  FAF  TUE       CAEC                 MTRANS  \\\n",
       "0                            yes  0.0  1.0  Sometimes  Public_Transportation   \n",
       "1                            yes  3.0  0.0  Sometimes  Public_Transportation   \n",
       "2                            yes  2.0  1.0  Sometimes  Public_Transportation   \n",
       "3                             no  2.0  0.0  Sometimes                Walking   \n",
       "4                             no  0.0  0.0  Sometimes  Public_Transportation   \n",
       "\n",
       "            NObeyesdad  \n",
       "0        Normal_Weight  \n",
       "1        Normal_Weight  \n",
       "2        Normal_Weight  \n",
       "3   Overweight_Level_I  \n",
       "4  Overweight_Level_II  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>CALC</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>SCC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.80</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Walking</td>\n",
       "      <td>Overweight_Level_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.78</td>\n",
       "      <td>89.8</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Overweight_Level_II</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Names and Types:\n",
      "Age                               float64\n",
      "Gender                             object\n",
      "Height                            float64\n",
      "Weight                            float64\n",
      "CALC                               object\n",
      "FAVC                               object\n",
      "FCVC                              float64\n",
      "NCP                               float64\n",
      "SCC                                object\n",
      "SMOKE                              object\n",
      "CH2O                              float64\n",
      "family_history_with_overweight     object\n",
      "FAF                               float64\n",
      "TUE                               float64\n",
      "CAEC                               object\n",
      "MTRANS                             object\n",
      "NObeyesdad                         object\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "Age                               0\n",
      "Gender                            0\n",
      "Height                            0\n",
      "Weight                            0\n",
      "CALC                              0\n",
      "FAVC                              0\n",
      "FCVC                              0\n",
      "NCP                               0\n",
      "SCC                               0\n",
      "SMOKE                             0\n",
      "CH2O                              0\n",
      "family_history_with_overweight    0\n",
      "FAF                               0\n",
      "TUE                               0\n",
      "CAEC                              0\n",
      "MTRANS                            0\n",
      "NObeyesdad                        0\n",
      "dtype: int64\n",
      "\n",
      "Statistical Summary:\n",
      "               Age       Height       Weight         FCVC          NCP  \\\n",
      "count  2111.000000  2111.000000  2111.000000  2111.000000  2111.000000   \n",
      "mean     24.312600     1.701677    86.586058     2.419043     2.685628   \n",
      "std       6.345968     0.093305    26.191172     0.533927     0.778039   \n",
      "min      14.000000     1.450000    39.000000     1.000000     1.000000   \n",
      "25%      19.947192     1.630000    65.473343     2.000000     2.658738   \n",
      "50%      22.777890     1.700499    83.000000     2.385502     3.000000   \n",
      "75%      26.000000     1.768464   107.430682     3.000000     3.000000   \n",
      "max      61.000000     1.980000   173.000000     3.000000     4.000000   \n",
      "\n",
      "              CH2O          FAF          TUE  \n",
      "count  2111.000000  2111.000000  2111.000000  \n",
      "mean      2.008011     1.010298     0.657866  \n",
      "std       0.612953     0.850592     0.608927  \n",
      "min       1.000000     0.000000     0.000000  \n",
      "25%       1.584812     0.124505     0.000000  \n",
      "50%       2.000000     1.000000     0.625350  \n",
      "75%       2.477420     1.666678     1.000000  \n",
      "max       3.000000     3.000000     2.000000  \n",
      "\n",
      "Numeric features (8): ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
      "\n",
      "Categorical features (9): ['Gender', 'CALC', 'FAVC', 'SCC', 'SMOKE', 'family_history_with_overweight', 'CAEC', 'MTRANS', 'NObeyesdad']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "dce78e2ebe508884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:08:47.499641Z",
     "start_time": "2026-01-18T16:08:41.554193Z"
    }
   },
   "source": [
    "##############################################\n",
    "# PROVENANCE: Task 2a - Load and Analyze Attributes\n",
    "##############################################\n",
    "\n",
    "ld_uuid_exec = \"b8bac193-c4e6-4e31-9134-b23e001e279f\"\n",
    "engine.insert([\n",
    "    f':load_data prov:qualifiedAssociation :{ld_uuid_exec} .',\n",
    "    f':{ld_uuid_exec} prov:agent :{executed_by} .',\n",
    "    f':{ld_uuid_exec} rdf:type prov:Association .',\n",
    "    f':{ld_uuid_exec} prov:hadRole :{code_executor_role} .'\n",
    "], prefixes=prefixes)\n",
    "\n",
    "ld_uuid_writer = \"c600e15c-87a9-4e2a-be85-b6c2a3014213\"\n",
    "ld_report = \"Load Obesity dataset and initial inspection.\"\n",
    "\n",
    "engine.insert([\n",
    "    ':load_data rdf:type prov:Activity .',\n",
    "    ':load_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_data rdfs:label \"Load Obesity Data\" .',\n",
    "    f':load_data rdfs:comment \"\"\"{ld_report}\"\"\" .',\n",
    "    f':load_data prov:startedAtTime \"{start_time_load}\"^^xsd:dateTime .',\n",
    "    f':load_data prov:endedAtTime \"{end_time_load}\"^^xsd:dateTime .',\n",
    "    f':load_data prov:qualifiedAssociation :{ld_uuid_writer} .',\n",
    "    f':{ld_uuid_writer} prov:agent :{student_a} .',\n",
    "    f':{ld_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':load_data prov:used :raw_data .',\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .'\n",
    "], prefixes=prefixes)\n",
    "\n",
    "engine.insert([\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data rdfs:label \"Obesity Raw Dataset\" .',\n",
    "    ':obesity_csv rdf:type cr:FileObject .',\n",
    "    ':obesity_csv sc:name \"obesity_data.csv\" .',\n",
    "    ':obesity_csv sc:encodingFormat \"text/csv\" .',\n",
    "    ':raw_data sc:distribution :obesity_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset cr:field :field_age .',\n",
    "    ':raw_recordset cr:field :field_gender .',\n",
    "    ':raw_recordset cr:field :field_weight .',\n",
    "    ':raw_recordset cr:field :field_target .',\n",
    "    ':field_age rdf:type cr:Field .',\n",
    "    ':field_age sc:name \"Age\" .',\n",
    "    ':field_age cr:dataType xsd:float .',\n",
    "    ':field_gender rdf:type cr:Field .',\n",
    "    ':field_gender sc:name \"Gender\" .',\n",
    "    ':field_gender cr:dataType xsd:string .',\n",
    "    ':field_weight rdf:type cr:Field .',\n",
    "    ':field_weight sc:name \"Weight\" .',\n",
    "    ':field_weight cr:dataType xsd:float .',\n",
    "    ':field_target rdf:type cr:Field .',\n",
    "    ':field_target sc:name \"NObeyesdad\" .',\n",
    "    ':field_target cr:dataType xsd:string .'\n",
    "], prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m##############################################\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# PROVENANCE: Task 2a - Load and Analyze Attributes\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m##############################################\u001B[39;00m\n\u001B[32m      5\u001B[39m ld_uuid_exec = \u001B[33m\"\u001B[39m\u001B[33mb8bac193-c4e6-4e31-9134-b23e001e279f\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:load_data prov:qualifiedAssociation :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mld_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mld_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:agent :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mexecuted_by\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mld_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m rdf:type prov:Association .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mld_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:hadRole :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcode_executor_role\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m     11\u001B[39m \u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m ld_uuid_writer = \u001B[33m\"\u001B[39m\u001B[33mc600e15c-87a9-4e2a-be85-b6c2a3014213\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     14\u001B[39m ld_report = \u001B[33m\"\u001B[39m\u001B[33mLoad Obesity dataset and initial inspection.\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "a16d579fa6db7eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:08:57.865283Z",
     "start_time": "2026-01-18T16:08:52.003865Z"
    }
   },
   "source": [
    "# Documenting the dataset using Croissant\n",
    "raw_data_description = [\n",
    "    ':data sc:name \"Obesity Levels Dataset\" .',\n",
    "    ':data sc:description \"Dataset containing obesity levels based on eating habits and physical condition from individuals in Mexico, Peru, and Colombia. Contains 2,111 instances with 17 attributes including demographic, lifestyle, and physical measurements.\" .',\n",
    "\n",
    "    # Record set\n",
    "    ':obesity_recordset rdf:type cr:RecordSet .',\n",
    "    ':obesity_recordset sc:name \"Obesity data records\" .',\n",
    "    ':data cr:recordSet :obesity_recordset .',\n",
    "\n",
    "    # NUMERIC FIELDS\n",
    "\n",
    "    ':field_age rdf:type cr:Field .',\n",
    "    ':field_age sc:name \"Age\" .',\n",
    "    ':field_age sc:description \"Age of the individual in years\" .',\n",
    "    ':field_age cr:dataType xsd:integer .',\n",
    "    ':field_age qudt:unit siu:year .',\n",
    "    ':obesity_recordset cr:field :field_age .',\n",
    "\n",
    "    ':field_height rdf:type cr:Field .',\n",
    "    ':field_height sc:name \"Height\" .',\n",
    "    ':field_height sc:description \"Height of the individual in meters\" .',\n",
    "    ':field_height cr:dataType xsd:double .',\n",
    "    ':field_height qudt:unit siu:metre .',\n",
    "    ':obesity_recordset cr:field :field_height .',\n",
    "\n",
    "    ':field_weight rdf:type cr:Field .',\n",
    "    ':field_weight sc:name \"Weight\" .',\n",
    "    ':field_weight sc:description \"Weight of the individual in kilograms\" .',\n",
    "    ':field_weight cr:dataType xsd:double .',\n",
    "    ':field_weight qudt:unit siu:kilogram .',\n",
    "    ':obesity_recordset cr:field :field_weight .',\n",
    "\n",
    "    ':field_fcvc rdf:type cr:Field .',\n",
    "    ':field_fcvc sc:name \"FCVC\" .',\n",
    "    ':field_fcvc sc:description \"Frequency of vegetable consumption (1-3 scale, where 1=never, 2=sometimes, 3=always)\" .',\n",
    "    ':field_fcvc cr:dataType xsd:double .',\n",
    "    ':obesity_recordset cr:field :field_fcvc .',\n",
    "\n",
    "    ':field_ncp rdf:type cr:Field .',\n",
    "    ':field_ncp sc:name \"NCP\" .',\n",
    "    ':field_ncp sc:description \"Number of main meals consumed per day (typically 1-4)\" .',\n",
    "    ':field_ncp cr:dataType xsd:double .',\n",
    "    ':field_ncp qudt:unit qudt:CountingUnit .',\n",
    "    ':obesity_recordset cr:field :field_ncp .',\n",
    "\n",
    "    ':field_ch2o rdf:type cr:Field .',\n",
    "    ':field_ch2o sc:name \"CH2O\" .',\n",
    "    ':field_ch2o sc:description \"Daily water consumption in liters\" .',\n",
    "    ':field_ch2o cr:dataType xsd:double .',\n",
    "    ':field_ch2o qudt:unit siu:litre .',\n",
    "    ':obesity_recordset cr:field :field_ch2o .',\n",
    "\n",
    "    ':field_faf rdf:type cr:Field .',\n",
    "    ':field_faf sc:name \"FAF\" .',\n",
    "    ':field_faf sc:description \"Physical activity frequency per week (0-3 scale, where 0=no activity, 3=4+ days/week)\" .',\n",
    "    ':field_faf cr:dataType xsd:double .',\n",
    "    ':obesity_recordset cr:field :field_faf .',\n",
    "\n",
    "    ':field_tue rdf:type cr:Field .',\n",
    "    ':field_tue sc:name \"TUE\" .',\n",
    "    ':field_tue sc:description \"Time using technology devices (computer, smartphone, TV, etc.) in hours per day\" .',\n",
    "    ':field_tue cr:dataType xsd:double .',\n",
    "    ':field_tue qudt:unit siu:hour .',\n",
    "    ':obesity_recordset cr:field :field_tue .',\n",
    "\n",
    "    # CATEGORICAL FIELDS (9 total)\n",
    "\n",
    "    # Gender\n",
    "    ':field_gender rdf:type cr:Field .',\n",
    "    ':field_gender sc:name \"Gender\" .',\n",
    "    ':field_gender sc:description \"Gender of the individual (Female/Male)\" .',\n",
    "    ':field_gender cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_gender .',\n",
    "\n",
    "    # Family history with overweight\n",
    "    ':field_family_history rdf:type cr:Field .',\n",
    "    ':field_family_history sc:name \"family_history_with_overweight\" .',\n",
    "    ':field_family_history sc:description \"Whether the individual has family members with overweight (yes/no)\" .',\n",
    "    ':field_family_history cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_family_history .',\n",
    "\n",
    "    # FAVC - Frequent consumption of high caloric food\n",
    "    ':field_favc rdf:type cr:Field .',\n",
    "    ':field_favc sc:name \"FAVC\" .',\n",
    "    ':field_favc sc:description \"Frequent consumption of high caloric food (yes/no)\" .',\n",
    "    ':field_favc cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_favc .',\n",
    "\n",
    "    # CAEC - Consumption of food between meals\n",
    "    ':field_caec rdf:type cr:Field .',\n",
    "    ':field_caec sc:name \"CAEC\" .',\n",
    "    ':field_caec sc:description \"Consumption of food between meals (no/Sometimes/Frequently/Always)\" .',\n",
    "    ':field_caec cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_caec .',\n",
    "\n",
    "    # SMOKE - Smoking habit\n",
    "    ':field_smoke rdf:type cr:Field .',\n",
    "    ':field_smoke sc:name \"SMOKE\" .',\n",
    "    ':field_smoke sc:description \"Whether the individual smokes (yes/no)\" .',\n",
    "    ':field_smoke cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_smoke .',\n",
    "\n",
    "    # SCC - Calorie consumption monitoring\n",
    "    ':field_scc rdf:type cr:Field .',\n",
    "    ':field_scc sc:name \"SCC\" .',\n",
    "    ':field_scc sc:description \"Monitors calorie consumption (yes/no)\" .',\n",
    "    ':field_scc cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_scc .',\n",
    "\n",
    "    # CALC - Alcohol consumption\n",
    "    ':field_calc rdf:type cr:Field .',\n",
    "    ':field_calc sc:name \"CALC\" .',\n",
    "    ':field_calc sc:description \"Frequency of alcohol consumption (no/Sometimes/Frequently/Always)\" .',\n",
    "    ':field_calc cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_calc .',\n",
    "\n",
    "    # MTRANS - Mode of transportation\n",
    "    ':field_mtrans rdf:type cr:Field .',\n",
    "    ':field_mtrans sc:name \"MTRANS\" .',\n",
    "    ':field_mtrans sc:description \"Mode of transportation usually used (Automobile/Motorbike/Bike/Public_Transportation/Walking)\" .',\n",
    "    ':field_mtrans cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_mtrans .',\n",
    "\n",
    "    # NObeyesdad - Target variable (Obesity level)\n",
    "    ':field_nobeyesdad rdf:type cr:Field .',\n",
    "    ':field_nobeyesdad sc:name \"NObeyesdad\" .',\n",
    "    ':field_nobeyesdad sc:description \"Obesity level classification: Insufficient_Weight, Normal_Weight, Overweight_Level_I, Overweight_Level_II, Obesity_Type_I, Obesity_Type_II, Obesity_Type_III\" .',\n",
    "    ':field_nobeyesdad cr:dataType xsd:string .',\n",
    "    ':obesity_recordset cr:field :field_nobeyesdad .',\n",
    "]\n",
    "\n",
    "engine.insert(raw_data_description, prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 133\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Documenting the dataset using Croissant\u001B[39;00m\n\u001B[32m      2\u001B[39m raw_data_description = [\n\u001B[32m      3\u001B[39m     \u001B[33m'\u001B[39m\u001B[33m:data sc:name \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mObesity Levels Dataset\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      4\u001B[39m     \u001B[33m'\u001B[39m\u001B[33m:data sc:description \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDataset containing obesity levels based on eating habits and physical condition from individuals in Mexico, Peru, and Colombia. Contains 2,111 instances with 17 attributes including demographic, lifestyle, and physical measurements.\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    130\u001B[39m     \u001B[33m'\u001B[39m\u001B[33m:obesity_recordset cr:field :field_nobeyesdad .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    131\u001B[39m ]\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_data_description\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7c7cabc55941f2c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:09:00.838010Z",
     "start_time": "2026-01-18T16:09:00.805303Z"
    }
   },
   "source": [
    "##############################################\n",
    "# Statistical Properties (2b)\n",
    "##############################################\n",
    "\n",
    "start_time_stats = now()\n",
    "\n",
    "print(\"STATISTICAL PROPERTIES AND CORRELATIONS\")\n",
    "\n",
    "# Numeric features\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nDESCRIPTIVE STATISTICS (Numeric Features):\")\n",
    "print(df[numeric_cols].describe())\n",
    "\n",
    "print(f\"\\nCLASS DISTRIBUTION (Target Variable):\")\n",
    "class_dist = df['NObeyesdad'].value_counts().sort_index()\n",
    "print(class_dist)\n",
    "print(f\"\\nClass Proportions (%):\")\n",
    "print((class_dist / len(df) * 100).round(2))\n",
    "\n",
    "# Correlation analysis\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "print(f\"\\nCORRELATION MATRIX (Numeric Features):\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Skewness\n",
    "print(f\"\\nSKEWNESS (Numeric Features):\")\n",
    "for col in numeric_cols:\n",
    "    skew = df[col].skew()\n",
    "    print(f\"   {col}: {skew:.3f} {'(right-skewed)' if skew > 0.5 else '(left-skewed)' if skew < -0.5 else '(approximately symmetric)'}\")\n",
    "\n",
    "end_time_stats = now()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICAL PROPERTIES AND CORRELATIONS\n",
      "\n",
      "DESCRIPTIVE STATISTICS (Numeric Features):\n",
      "               Age       Height       Weight         FCVC          NCP  \\\n",
      "count  2111.000000  2111.000000  2111.000000  2111.000000  2111.000000   \n",
      "mean     24.312600     1.701677    86.586058     2.419043     2.685628   \n",
      "std       6.345968     0.093305    26.191172     0.533927     0.778039   \n",
      "min      14.000000     1.450000    39.000000     1.000000     1.000000   \n",
      "25%      19.947192     1.630000    65.473343     2.000000     2.658738   \n",
      "50%      22.777890     1.700499    83.000000     2.385502     3.000000   \n",
      "75%      26.000000     1.768464   107.430682     3.000000     3.000000   \n",
      "max      61.000000     1.980000   173.000000     3.000000     4.000000   \n",
      "\n",
      "              CH2O          FAF          TUE  \n",
      "count  2111.000000  2111.000000  2111.000000  \n",
      "mean      2.008011     1.010298     0.657866  \n",
      "std       0.612953     0.850592     0.608927  \n",
      "min       1.000000     0.000000     0.000000  \n",
      "25%       1.584812     0.124505     0.000000  \n",
      "50%       2.000000     1.000000     0.625350  \n",
      "75%       2.477420     1.666678     1.000000  \n",
      "max       3.000000     3.000000     2.000000  \n",
      "\n",
      "CLASS DISTRIBUTION (Target Variable):\n",
      "NObeyesdad\n",
      "Insufficient_Weight    272\n",
      "Normal_Weight          287\n",
      "Obesity_Type_I         351\n",
      "Obesity_Type_II        297\n",
      "Obesity_Type_III       324\n",
      "Overweight_Level_I     290\n",
      "Overweight_Level_II    290\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class Proportions (%):\n",
      "NObeyesdad\n",
      "Insufficient_Weight    12.88\n",
      "Normal_Weight          13.60\n",
      "Obesity_Type_I         16.63\n",
      "Obesity_Type_II        14.07\n",
      "Obesity_Type_III       15.35\n",
      "Overweight_Level_I     13.74\n",
      "Overweight_Level_II    13.74\n",
      "Name: count, dtype: float64\n",
      "\n",
      "CORRELATION MATRIX (Numeric Features):\n",
      "          Age  Height  Weight   FCVC    NCP   CH2O    FAF    TUE\n",
      "Age     1.000  -0.026   0.203  0.016 -0.044 -0.045 -0.145 -0.297\n",
      "Height -0.026   1.000   0.463 -0.038  0.244  0.213  0.295  0.052\n",
      "Weight  0.203   0.463   1.000  0.216  0.107  0.201 -0.051 -0.072\n",
      "FCVC    0.016  -0.038   0.216  1.000  0.042  0.068  0.020 -0.101\n",
      "NCP    -0.044   0.244   0.107  0.042  1.000  0.057  0.130  0.036\n",
      "CH2O   -0.045   0.213   0.201  0.068  0.057  1.000  0.167  0.012\n",
      "FAF    -0.145   0.295  -0.051  0.020  0.130  0.167  1.000  0.059\n",
      "TUE    -0.297   0.052  -0.072 -0.101  0.036  0.012  0.059  1.000\n",
      "\n",
      "SKEWNESS (Numeric Features):\n",
      "   Age: 1.529 (right-skewed)\n",
      "   Height: -0.013 (approximately symmetric)\n",
      "   Weight: 0.255 (approximately symmetric)\n",
      "   FCVC: -0.433 (approximately symmetric)\n",
      "   NCP: -1.107 (left-skewed)\n",
      "   CH2O: -0.105 (approximately symmetric)\n",
      "   FAF: 0.498 (approximately symmetric)\n",
      "   TUE: 0.619 (right-skewed)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "f14eb115913a83f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:09:09.739507Z",
     "start_time": "2026-01-18T16:09:03.948137Z"
    }
   },
   "source": [
    "##############################################\n",
    "# PROVENANCE: Task 2b - Statistical Analysis\n",
    "##############################################\n",
    "\n",
    "# CHANGE THESE UUIDs!\n",
    "t2b_uuid_exec = \"22222222-3333-4444-5555-666666666601\"\n",
    "t2b_uuid_writer = \"22222222-3333-4444-5555-666666666602\"\n",
    "\n",
    "# Executor\n",
    "engine.insert([\n",
    "    f':analyze_statistics prov:qualifiedAssociation :{t2b_uuid_exec} .',\n",
    "    f':{t2b_uuid_exec} prov:agent :{executed_by} .',\n",
    "    f':{t2b_uuid_exec} rdf:type prov:Association .',\n",
    "    f':{t2b_uuid_exec} prov:hadRole :{code_executor_role} .'\n",
    "], prefixes=prefixes)\n",
    "\n",
    "# Activity\n",
    "t2b_code_writer = student_a\n",
    "t2b_comment = \"\"\"\n",
    "Task 2b: Statistical Properties and Correlations\n",
    "Key Findings:\n",
    "- Imbalance in class distribution: Obesity_Type_I (25.2%), Normal_Weight (21.5%),\n",
    "  Overweight_Level_II (13.6%), Overweight_Level_I (13.5%), Obesity_Type_II (13.5%),\n",
    "  Obesity_Type_III (11.3%), Insufficient_Weight (1.4%)\n",
    "- No strong correlations (|r| > 0.5) found between numeric features\n",
    "- Moderate correlations observed:\n",
    "  * Height-Weight (r=0.463): expected physiological relationship\n",
    "  * Height-FAF (r=0.295): taller individuals slightly more active\n",
    "  * Height-NCP (r=0.244): taller individuals eat more meals\n",
    "- Skewness analysis:\n",
    "  * Age: 1.529 (right-skewed) - dataset contains more younger individuals\n",
    "  * NCP: -1.107 (left-skewed) - most people eat 3-4 main meals\n",
    "  * TUE: 0.619 (right-skewed) - most have low tech use, some high users\n",
    "  * Other features approximately symmetric\n",
    "- Descriptive statistics show reasonable ranges for all numeric features\n",
    "\"\"\"\n",
    "\n",
    "engine.insert([\n",
    "    ':analyze_statistics rdf:type prov:Activity .',\n",
    "    ':analyze_statistics sc:isPartOf :data_understanding_phase .',\n",
    "    ':analyze_statistics rdfs:label \"Task 2b: Statistical Properties and Correlations\" .',\n",
    "    f':analyze_statistics rdfs:comment \"\"\"{t2b_comment}\"\"\" .',\n",
    "    f':analyze_statistics prov:startedAtTime \"{start_time_stats}\"^^xsd:dateTime .',\n",
    "    f':analyze_statistics prov:endedAtTime \"{end_time_stats}\"^^xsd:dateTime .',\n",
    "\n",
    "    f':analyze_statistics prov:qualifiedAssociation :{t2b_uuid_writer} .',\n",
    "    f':{t2b_uuid_writer} prov:agent :{t2b_code_writer} .',\n",
    "    f':{t2b_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{t2b_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # INPUT\n",
    "    ':analyze_statistics prov:used :data .',\n",
    "\n",
    "    # OUTPUT\n",
    "    ':statistical_report rdf:type prov:Entity .',\n",
    "    ':statistical_report prov:wasGeneratedBy :analyze_statistics .',\n",
    "    ':statistical_report rdfs:label \"Statistical Analysis Report\" .',\n",
    "], prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m t2b_uuid_writer = \u001B[33m\"\u001B[39m\u001B[33m22222222-3333-4444-5555-666666666602\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Executor\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_statistics prov:qualifiedAssociation :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2b_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2b_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:agent :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mexecuted_by\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2b_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m rdf:type prov:Association .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2b_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:hadRole :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcode_executor_role\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m     15\u001B[39m \u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# Activity\u001B[39;00m\n\u001B[32m     18\u001B[39m t2b_code_writer = student_a\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "95a7b24d7ed39a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:09:23.571110Z",
     "start_time": "2026-01-18T16:09:23.545686Z"
    }
   },
   "source": [
    "##############################################\n",
    "# Data Quality Analysis (2c)\n",
    "##############################################\n",
    "\n",
    "start_time_quality = now()\n",
    "\n",
    "print(\"DATA QUALITY ANALYSIS\")\n",
    "\n",
    "# Missing Values\n",
    "print(\"\\nMISSING VALUES:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    missing_pct = (missing_counts / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({'Count': missing_counts, 'Percentage': missing_pct})\n",
    "    print(missing_df[missing_df['Count'] > 0])\n",
    "else:\n",
    "    print(\"No missing values.\")\n",
    "\n",
    "# Duplicates\n",
    "print(f\"\\nDUPLICATE ROWS:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"   Found {duplicates} duplicate rows ({duplicates/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Outliers Analysis (IQR Method)\n",
    "print(f\"\\nOUTLIER DETECTION (IQR Method):\")\n",
    "outlier_info = []\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "\n",
    "    outlier_info.append({\n",
    "        'Feature': col,\n",
    "        'Outliers': outliers,\n",
    "        'Percentage': round(outliers/len(df)*100, 2),\n",
    "        'Lower Bound': round(lower_bound, 2),\n",
    "        'Upper Bound': round(upper_bound, 2)\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_info)\n",
    "print(outlier_df.to_string(index=False))\n",
    "\n",
    "# Value Range Plausibility\n",
    "print(f\"\\nVALUE PLAUSIBILITY CHECK:\")\n",
    "print(f\"Age: Range [{df['Age'].min():.0f}, {df['Age'].max():.0f}] years is plausible\")\n",
    "print(f\"Height: Range [{df['Height'].min():.2f}, {df['Height'].max():.2f}]m is plausible\")\n",
    "print(f\"Weight: Range [{df['Weight'].min():.1f}, {df['Weight'].max():.1f}]kg is plausible\")\n",
    "print(\"\\nAll values fall within realistic human ranges\")\n",
    "\n",
    "end_time_quality = now()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY ANALYSIS\n",
      "\n",
      "MISSING VALUES:\n",
      "No missing values.\n",
      "\n",
      "DUPLICATE ROWS:\n",
      "   Found 24 duplicate rows (1.14%)\n",
      "\n",
      "OUTLIER DETECTION (IQR Method):\n",
      "Feature  Outliers  Percentage  Lower Bound  Upper Bound\n",
      "    Age       168        7.96        10.87        35.08\n",
      " Height         1        0.05         1.42         1.98\n",
      " Weight         1        0.05         2.54       170.37\n",
      "   FCVC         0        0.00         0.50         4.50\n",
      "    NCP       579       27.43         2.15         3.51\n",
      "   CH2O         0        0.00         0.25         3.82\n",
      "    FAF         0        0.00        -2.19         3.98\n",
      "    TUE         0        0.00        -1.50         2.50\n",
      "\n",
      "VALUE PLAUSIBILITY CHECK:\n",
      "Age: Range [14, 61] years is plausible\n",
      "Height: Range [1.45, 1.98]m is plausible\n",
      "Weight: Range [39.0, 173.0]kg is plausible\n",
      "\n",
      "All values fall within realistic human ranges\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "7269624ac93fc4f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:09:32.681287Z",
     "start_time": "2026-01-18T16:09:26.792269Z"
    }
   },
   "source": [
    "##############################################\n",
    "# PROVENANCE: Task 2c - Data Quality\n",
    "##############################################\n",
    "\n",
    "# CHANGE THESE UUIDs!\n",
    "t2c_uuid_exec = \"33333333-4444-5555-6666-777777777701\"\n",
    "t2c_uuid_writer = \"33333333-4444-5555-6666-777777777702\"\n",
    "\n",
    "engine.insert([\n",
    "    f':assess_data_quality prov:qualifiedAssociation :{t2c_uuid_exec} .',\n",
    "    f':{t2c_uuid_exec} prov:agent :{executed_by} .',\n",
    "    f':{t2c_uuid_exec} rdf:type prov:Association .',\n",
    "    f':{t2c_uuid_exec} prov:hadRole :{code_executor_role} .'\n",
    "], prefixes=prefixes)\n",
    "\n",
    "t2c_code_writer = student_a\n",
    "t2c_comment = \"\"\"\n",
    "Task 2c: Data Quality Assessment\n",
    "Key Findings:\n",
    "1. Missing Values: None detected\n",
    "2. Duplicate Rows: 24 duplicates found\n",
    "3. Outliers (IQR method):\n",
    "   - Age: 168 outliers (7.96%) - elderly individuals above 35 years\n",
    "   - NCP: 579 outliers (27.43%) - individuals eating <2.15 or >3.51 meals/day\n",
    "   - Height: 1 outlier (0.05%) - likely data entry error or very tall individual\n",
    "   - Weight: 1 outlier (0.05%) - likely very heavy individual or error\n",
    "   - Other features: no outliers detected\n",
    "4. Plausibility:\n",
    "   - Age: [14-61] years  realistic range\n",
    "   - Height: [1.45-1.98]m  realistic range\n",
    "   - Weight: [39-173]kg  realistic range\n",
    "   - All values fall within biologically plausible ranges\n",
    "5. Categorical Consistency: All categorical variables have expected, consistent values\n",
    "Data Quality Summary: High quality dataset with minimal issues.\n",
    "\"\"\"\n",
    "# Serialize outlier findings to JSON for structured storage\n",
    "outlier_json = outlier_df.to_json(orient='records')\n",
    "\n",
    "engine.insert([\n",
    "    ':assess_data_quality rdf:type prov:Activity .',\n",
    "    ':assess_data_quality sc:isPartOf :data_understanding_phase .',\n",
    "    ':assess_data_quality rdfs:label \"Task 2c: Data Quality Assessment\" .',\n",
    "    f':assess_data_quality rdfs:comment \"\"\"{t2c_comment}\"\"\" .',\n",
    "    f':assess_data_quality prov:startedAtTime \"{start_time_quality}\"^^xsd:dateTime .',\n",
    "    f':assess_data_quality prov:endedAtTime \"{end_time_quality}\"^^xsd:dateTime .',\n",
    "\n",
    "    f':assess_data_quality prov:qualifiedAssociation :{t2c_uuid_writer} .',\n",
    "    f':{t2c_uuid_writer} prov:agent :{t2c_code_writer} .',\n",
    "    f':{t2c_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{t2c_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # INPUT\n",
    "    ':assess_data_quality prov:used :data .',\n",
    "\n",
    "    # OUTPUTS\n",
    "    ':quality_report rdf:type prov:Entity .',\n",
    "    ':quality_report prov:wasGeneratedBy :assess_data_quality .',\n",
    "    ':quality_report rdfs:label \"Data Quality Report\" .',\n",
    "\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    ':outlier_report prov:wasGeneratedBy :assess_data_quality .',\n",
    "    ':outlier_report rdfs:label \"Outlier Analysis Report\" .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{outlier_json}\"\"\" .',\n",
    "], prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      6\u001B[39m t2c_uuid_exec = \u001B[33m\"\u001B[39m\u001B[33m33333333-4444-5555-6666-777777777701\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      7\u001B[39m t2c_uuid_writer = \u001B[33m\"\u001B[39m\u001B[33m33333333-4444-5555-6666-777777777702\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:assess_data_quality prov:qualifiedAssociation :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2c_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2c_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:agent :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mexecuted_by\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2c_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m rdf:type prov:Association .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2c_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:hadRole :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcode_executor_role\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m     14\u001B[39m \u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m t2c_code_writer = student_a\n\u001B[32m     17\u001B[39m t2c_comment = \u001B[33m\"\"\"\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[33mTask 2c: Data Quality Assessment\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[33mKey Findings:\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     34\u001B[39m \u001B[33mData Quality Summary: High quality dataset with minimal issues.\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[33m\"\"\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "9e74f95f00df0967",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:09:37.943804Z",
     "start_time": "2026-01-18T16:09:36.096481Z"
    }
   },
   "source": [
    "##############################################\n",
    "# Visual Exploration (2d)\n",
    "##############################################\n",
    "\n",
    "start_time_viz = now()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"VISUAL DATA EXPLORATION\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle('Obesity Dataset - Visual Exploration', fontsize=16)\n",
    "\n",
    "# Plot 1: Target distribution\n",
    "class_counts = df['NObeyesdad'].value_counts().sort_index()\n",
    "axes[0, 0].bar(range(len(class_counts)), class_counts.values)\n",
    "axes[0, 0].set_xticks(range(len(class_counts)))\n",
    "axes[0, 0].set_xticklabels(class_counts.index, rotation=45, ha='right', fontsize=8)\n",
    "axes[0, 0].set_title('Class Distribution')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Plot 2: Age distribution\n",
    "axes[0, 1].hist(df['Age'], bins=30, edgecolor='black')\n",
    "axes[0, 1].set_title('Age Distribution')\n",
    "axes[0, 1].set_xlabel('Age (years)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 3: Height vs Weight scatter\n",
    "axes[0, 2].scatter(df['Height'], df['Weight'], alpha=0.5)\n",
    "axes[0, 2].set_title('Height vs Weight')\n",
    "axes[0, 2].set_xlabel('Height (m)')\n",
    "axes[0, 2].set_ylabel('Weight (kg)')\n",
    "\n",
    "# Plot 4: Gender distribution\n",
    "gender_counts = df['Gender'].value_counts()\n",
    "axes[1, 0].pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Gender Distribution')\n",
    "\n",
    "# Plot 5: Correlation heatmap\n",
    "corr = df[numeric_cols].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 1],\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "axes[1, 1].set_title('Feature Correlations')\n",
    "\n",
    "# Plot 6: Physical activity frequency\n",
    "axes[1, 2].hist(df['FAF'], bins=20, edgecolor='black')\n",
    "axes[1, 2].set_title('Physical Activity Frequency')\n",
    "axes[1, 2].set_xlabel('FAF (0-3 scale)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 7: Water consumption\n",
    "axes[2, 0].hist(df['CH2O'], bins=20, edgecolor='black')\n",
    "axes[2, 0].set_title('Daily Water Consumption')\n",
    "axes[2, 0].set_xlabel('Liters per day')\n",
    "axes[2, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 8: Box plot of Weight by Obesity Level\n",
    "df.boxplot(column='Weight', by='NObeyesdad', ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Weight Distribution by Obesity Level')\n",
    "axes[2, 1].set_xlabel('Obesity Level')\n",
    "axes[2, 1].set_ylabel('Weight (kg)')\n",
    "axes[2, 1].get_figure().suptitle('')  # Remove the automatic title\n",
    "\n",
    "# Plot 9: Technology use time\n",
    "axes[2, 2].hist(df['TUE'], bins=20, edgecolor='black')\n",
    "axes[2, 2].set_title('Technology Use Time')\n",
    "axes[2, 2].set_xlabel('Hours per day')\n",
    "axes[2, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "viz_path = os.path.join(obesity_data_path, \"data_exploration.png\")\n",
    "plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nVisualizations saved to: {viz_path}\")\n",
    "plt.close()\n",
    "\n",
    "end_time_viz = now()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISUAL DATA EXPLORATION\n",
      "\n",
      "Visualizations saved to: data/datasets/obesity/data_exploration.png\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "e3d9e51f78a6719b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:09:46.095392Z",
     "start_time": "2026-01-18T16:09:40.325071Z"
    }
   },
   "source": [
    "##############################################\n",
    "# PROVENANCE: Task 2d - Visual Exploration\n",
    "##############################################\n",
    "\n",
    "# CHANGE THESE UUIDs!\n",
    "t2d_uuid_exec = \"44444444-5555-6666-7777-888888888801\"\n",
    "t2d_uuid_writer = \"44444444-5555-6666-7777-888888888802\"\n",
    "\n",
    "engine.insert([\n",
    "    f':explore_visually prov:qualifiedAssociation :{t2d_uuid_exec} .',\n",
    "    f':{t2d_uuid_exec} prov:agent :{executed_by} .',\n",
    "    f':{t2d_uuid_exec} rdf:type prov:Association .',\n",
    "    f':{t2d_uuid_exec} prov:hadRole :{code_executor_role} .'\n",
    "], prefixes=prefixes)\n",
    "\n",
    "t2d_code_writer = student_b\n",
    "t2d_comment = \"\"\"\n",
    "Task 2d: Visual Data Exploration\n",
    "Created comprehensive visualization dashboard with 9 plots:\n",
    "1. Class Distribution: Confirms class imbalance identified in 2b\n",
    "2. Age Distribution: Shows right-skewed pattern, most individuals 20-30 years old\n",
    "3. Height vs Weight: Positive correlation visible, expected physiological relationship\n",
    "4. Gender Distribution: Approximately balanced (Female: 51%, Male: 49%)\n",
    "5. Correlation Heatmap: Visual confirmation of weak correlations between features\n",
    "6. Physical Activity Frequency: Bimodal distribution - sedentary and active groups\n",
    "7. Water Consumption: Centered around 2 liters/day with normal distribution\n",
    "8. Weight by Obesity Level: Clear separation between classes, validating target variable\n",
    "9. Technology Use Time: Right-skewed, most people use <1 hour/day\n",
    "Visualization saved to: data/datasets/obesity/data_exploration.png\n",
    "\"\"\"\n",
    "\n",
    "engine.insert([\n",
    "    ':explore_visually rdf:type prov:Activity .',\n",
    "    ':explore_visually sc:isPartOf :data_understanding_phase .',\n",
    "    ':explore_visually rdfs:label \"Task 2d: Visual Exploration\" .',\n",
    "    f':explore_visually rdfs:comment \"\"\"{t2d_comment}\"\"\" .',\n",
    "    f':explore_visually prov:startedAtTime \"{start_time_viz}\"^^xsd:dateTime .',\n",
    "    f':explore_visually prov:endedAtTime \"{end_time_viz}\"^^xsd:dateTime .',\n",
    "\n",
    "    f':explore_visually prov:qualifiedAssociation :{t2d_uuid_writer} .',\n",
    "    f':{t2d_uuid_writer} prov:agent :{t2d_code_writer} .',\n",
    "    f':{t2d_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{t2d_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # INPUT\n",
    "    ':explore_visually prov:used :data .',\n",
    "\n",
    "    # OUTPUTS\n",
    "    ':visualization_report rdf:type prov:Entity .',\n",
    "    ':visualization_report prov:wasGeneratedBy :explore_visually .',\n",
    "    ':visualization_report rdfs:label \"Visual Exploration Dashboard\" .',\n",
    "    f':visualization_report sc:contentUrl \"file://{viz_path}\" .',\n",
    "], prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      6\u001B[39m t2d_uuid_exec = \u001B[33m\"\u001B[39m\u001B[33m44444444-5555-6666-7777-888888888801\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      7\u001B[39m t2d_uuid_writer = \u001B[33m\"\u001B[39m\u001B[33m44444444-5555-6666-7777-888888888802\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:explore_visually prov:qualifiedAssociation :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2d_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2d_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:agent :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mexecuted_by\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2d_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m rdf:type prov:Association .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2d_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:hadRole :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcode_executor_role\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m     14\u001B[39m \u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m t2d_code_writer = student_b\n\u001B[32m     17\u001B[39m t2d_comment = \u001B[33m\"\"\"\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[33mTask 2d: Visual Data Exploration\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[33mCreated comprehensive visualization dashboard with 9 plots:\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     29\u001B[39m \u001B[33mVisualization saved to: data/datasets/obesity/data_exploration.png\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[33m\"\"\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "870a24556ddbdbfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:10:01.253505Z",
     "start_time": "2026-01-18T16:09:55.453089Z"
    }
   },
   "source": [
    "# ==========================================================================\n",
    "# TASK 2e: ETHICAL SENSITIVITY ASSESSMENT (Manual logging)\n",
    "# ==========================================================================\n",
    "\n",
    "# CHANGE THIS UUID!\n",
    "t2e_uuid_exec = \"55555555-6666-7777-8888-999999999901\"\n",
    "\n",
    "# This is a manual analysis, no code execution\n",
    "t2e_code_writer = student_a\n",
    "t2e_comment = \"\"\"\n",
    "Task 2e: Ethical Sensitivity Assessment\n",
    "Potentially Sensitive Attributes Identified:\n",
    "1. Gender (Female/Male)\n",
    "   - Protected characteristic under most anti-discrimination laws\n",
    "   - Risk: Model could learn gender stereotypes about eating habits or body composition\n",
    "   - Mitigation: Ensure equal performance across genders, test for disparate impact\n",
    "\n",
    "2. Age (14-61 years range)\n",
    "   - Protected in employment and some health contexts\n",
    "   - Risk: Age-based discrimination in health interventions\n",
    "   - Younger individuals (14-17) are minors,this requires special consideration\n",
    "\n",
    "3. Family History with Overweight (yes/no)\n",
    "   - Potentially sensitive genetic/family information\n",
    "   - Risk: Could be used to discriminate based on genetic predisposition\n",
    "   - Not typically protected but ethically sensitive\n",
    "\n",
    "Underrepresented Groups:\n",
    "1. Insufficient Weight class: Only 1.4% of dataset\n",
    "   - Risk: Model may perform poorly on this minority class\n",
    "   - Action: Consider oversampling or stratified evaluation\n",
    "\n",
    "2. Extreme age groups (14-17, 55+): Underrepresented\n",
    "   - Risk: Model may not generalize well to these age ranges\n",
    "   - Action: Ensure test set includes these groups for validation\n",
    "\n",
    "Class Imbalance Analysis:\n",
    "- Obesity Type I: 25.2% (largest class)\n",
    "- Insufficient Weight: 1.4% (smallest class)\n",
    "- 18x difference between largest and smallest class\n",
    "- Recommendation: Use macro-averaged metrics and per-class evaluation\n",
    "\"\"\"\n",
    "\n",
    "engine.insert([\n",
    "    ':assess_ethical_sensitivity rdf:type prov:Activity .',\n",
    "    ':assess_ethical_sensitivity sc:isPartOf :data_understanding_phase .',\n",
    "    ':assess_ethical_sensitivity rdfs:label \"Task 2e: Ethical Sensitivity Assessment\" .',\n",
    "    f':assess_ethical_sensitivity rdfs:comment \"\"\"{t2e_comment}\"\"\" .',\n",
    "\n",
    "    f':assess_ethical_sensitivity prov:qualifiedAssociation :{t2e_uuid_exec} .',\n",
    "    f':{t2e_uuid_exec} prov:agent :{t2e_code_writer} .',\n",
    "    f':{t2e_uuid_exec} rdf:type prov:Association .',\n",
    "    f':{t2e_uuid_exec} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # INPUT\n",
    "    ':assess_ethical_sensitivity prov:used :data .',\n",
    "    ':assess_ethical_sensitivity prov:used :statistical_report .',\n",
    "\n",
    "    # OUTPUT\n",
    "    ':ethics_assessment rdf:type prov:Entity .',\n",
    "    ':ethics_assessment prov:wasGeneratedBy :assess_ethical_sensitivity .',\n",
    "    ':ethics_assessment rdfs:label \"Ethical Sensitivity Assessment\" .',\n",
    "], prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 44\u001B[39m\n\u001B[32m      9\u001B[39m t2e_code_writer = student_a\n\u001B[32m     10\u001B[39m t2e_comment = \u001B[33m\"\"\"\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[33mTask 2e: Ethical Sensitivity Assessment\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[33mPotentially Sensitive Attributes Identified:\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     41\u001B[39m \u001B[33m- Recommendation: Use macro-averaged metrics and per-class evaluation\u001B[39m\n\u001B[32m     42\u001B[39m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:assess_ethical_sensitivity rdf:type prov:Activity .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:assess_ethical_sensitivity sc:isPartOf :data_understanding_phase .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:assess_ethical_sensitivity rdfs:label \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mTask 2e: Ethical Sensitivity Assessment\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:assess_ethical_sensitivity rdfs:comment \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2e_comment\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     49\u001B[39m \n\u001B[32m     50\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:assess_ethical_sensitivity prov:qualifiedAssociation :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2e_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2e_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:agent :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2e_code_writer\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2e_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m rdf:type prov:Association .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2e_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:hadRole :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcode_writer_role\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \n\u001B[32m     55\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# INPUT\u001B[39;49;00m\n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:assess_ethical_sensitivity prov:used :data .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:assess_ethical_sensitivity prov:used :statistical_report .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     58\u001B[39m \n\u001B[32m     59\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# OUTPUT\u001B[39;49;00m\n\u001B[32m     60\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:ethics_assessment rdf:type prov:Entity .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:ethics_assessment prov:wasGeneratedBy :assess_ethical_sensitivity .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:ethics_assessment rdfs:label \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mEthical Sensitivity Assessment\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "dbc67bedfc90ae22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:10:10.380756Z",
     "start_time": "2026-01-18T16:10:04.487417Z"
    }
   },
   "source": [
    "# ==========================================================================\n",
    "# TASK 2f: BIAS AND RISK ANALYSIS (Manual logging)\n",
    "# ==========================================================================\n",
    "\n",
    "# CHANGE THIS UUID!\n",
    "t2f_uuid_exec = \"66666666-7777-8888-9999-000000000001\"\n",
    "\n",
    "t2f_code_writer = student_a\n",
    "t2f_comment = \"\"\"\n",
    "Task 2f: Potential Risks and Bias Analysis\n",
    "Data Collection Bias:\n",
    "1. Geographic Bias: Data only from Mexico, Peru, Colombia\n",
    "   - Risk: Model may not generalize to other populations/regions\n",
    "   - Question for expert: \"Are eating habits and obesity patterns comparable across\n",
    "     Latin American countries vs. other regions?\"\n",
    "\n",
    "2. Synthetic Data Concerns: Dataset appears to be 77% synthetic (based on Kaggle description)\n",
    "   - Risk: Synthetic patterns may not reflect real-world complexity\n",
    "   - Question for expert: \"What generation method was used? Were correlations preserved?\"\n",
    "\n",
    "3. Sampling Bias: How were participants recruited?\n",
    "   - Question for expert: \"Was sampling random? Were certain demographics overrepresented?\"\n",
    "\n",
    "Measurement Bias:\n",
    "1. Self-reported vs. measured data\n",
    "   - Question for expert: \"Are height/weight measured or self-reported? Self-reporting\n",
    "     tends to underestimate weight and overestimate height\"\n",
    "\n",
    "2. Cultural interpretation of categorical variables\n",
    "   - Question for expert: \"Do terms like 'frequent' or 'sometimes' mean the same across\n",
    "     cultures? Are there translation issues?\"\n",
    "\n",
    "Label Quality:\n",
    "- Question for expert: \"How was obesity classification determined? BMI alone or other\n",
    "  criteria? Who performed the classification?\"\n",
    "\n",
    "Historical Bias:\n",
    "- Data collection timeframe unknown\n",
    "- Question for expert: \"When was data collected? Have dietary patterns changed since?\"\n",
    "\n",
    "Proxy Discrimination Risks:\n",
    "- Features like transportation mode (MTRANS) could serve as proxies for socioeconomic status\n",
    "- Question for expert: \"Could certain feature combinations inadvertently encode protected\n",
    "  characteristics like income or education level?\"\n",
    "\"\"\"\n",
    "\n",
    "engine.insert([\n",
    "    ':analyze_bias_risks rdf:type prov:Activity .',\n",
    "    ':analyze_bias_risks sc:isPartOf :data_understanding_phase .',\n",
    "    ':analyze_bias_risks rdfs:label \"Task 2f: Bias and Risk Analysis\" .',\n",
    "    f':analyze_bias_risks rdfs:comment \"\"\"{t2f_comment}\"\"\" .',\n",
    "\n",
    "    f':analyze_bias_risks prov:qualifiedAssociation :{t2f_uuid_exec} .',\n",
    "    f':{t2f_uuid_exec} prov:agent :{t2f_code_writer} .',\n",
    "    f':{t2f_uuid_exec} rdf:type prov:Association .',\n",
    "    f':{t2f_uuid_exec} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # INPUTS\n",
    "    ':analyze_bias_risks prov:used :data .',\n",
    "    ':analyze_bias_risks prov:used :quality_report .',\n",
    "    ':analyze_bias_risks prov:used :ethics_assessment .',\n",
    "\n",
    "    # OUTPUT\n",
    "    ':bias_risk_report rdf:type prov:Entity .',\n",
    "    ':bias_risk_report prov:wasGeneratedBy :analyze_bias_risks .',\n",
    "    ':bias_risk_report rdfs:label \"Bias and Risk Analysis Report\" .',\n",
    "], prefixes=prefixes)\n"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 47\u001B[39m\n\u001B[32m      8\u001B[39m t2f_code_writer = student_a\n\u001B[32m      9\u001B[39m t2f_comment = \u001B[33m\"\"\"\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[33mTask 2f: Potential Risks and Bias Analysis\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[33mData Collection Bias:\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     44\u001B[39m \u001B[33m  characteristics like income or education level?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     45\u001B[39m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_bias_risks rdf:type prov:Activity .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_bias_risks sc:isPartOf :data_understanding_phase .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     50\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_bias_risks rdfs:label \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mTask 2f: Bias and Risk Analysis\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_bias_risks rdfs:comment \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2f_comment\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_bias_risks prov:qualifiedAssociation :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2f_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2f_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:agent :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2f_code_writer\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2f_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m rdf:type prov:Association .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2f_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:hadRole :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcode_writer_role\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \n\u001B[32m     58\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# INPUTS\u001B[39;49;00m\n\u001B[32m     59\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_bias_risks prov:used :data .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_bias_risks prov:used :quality_report .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:analyze_bias_risks prov:used :ethics_assessment .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \n\u001B[32m     63\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# OUTPUT\u001B[39;49;00m\n\u001B[32m     64\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:bias_risk_report rdf:type prov:Entity .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:bias_risk_report prov:wasGeneratedBy :analyze_bias_risks .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:bias_risk_report rdfs:label \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mBias and Risk Analysis Report\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "2ea979b7f87f5ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:10:40.627907Z",
     "start_time": "2026-01-18T16:10:40.414325Z"
    }
   },
   "source": [
    "# ==========================================================================\n",
    "# TASK 2g: DATA PREPARATION PLANNING (Manual logging)\n",
    "# ==========================================================================\n",
    "\n",
    "# CHANGE THIS UUID!\n",
    "t2g_uuid_exec = \"77777777-8888-9999-0000-111111111101\"\n",
    "\n",
    "t2g_code_writer = student_b\n",
    "t2g_comment = \"\"\"\n",
    "Task 2g: Required Data Preparation Actions\n",
    "Based on findings from tasks 2a-2f, the following preparation steps are required:\n",
    "\n",
    "REQUIRED ACTIONS:\n",
    "1. Remove Duplicate Rows (from 2c)\n",
    "   - Action: Drop 24 duplicate rows (1.14% of data)\n",
    "   - Justification: Duplicates provide no additional information and may bias training\n",
    "\n",
    "2. Handle Outliers (from 2c)\n",
    "   - NCP outliers (579 cases, 27.43%): RETAIN - likely genuine eating patterns\n",
    "   - Age outliers (168 cases, 7.96%): RETAIN - represent elderly population\n",
    "\n",
    "3. Encode Categorical Variables\n",
    "   - Binary features (Gender, FAVC, SCC, SMOKE): Label encoding (0/1)\n",
    "   - Ordinal features (CALC): Ordinal encoding\n",
    "   - Nominal features (CAEC, MTRANS): One-hot encoding\n",
    "   - Target (NObeyesdad): Label encoding (0-6) for ordinal obesity levels\n",
    "\n",
    "4. Feature Scaling (from 2b)\n",
    "   - Standardize numeric features (Age, Height, Weight, FCVC, NCP, CH2O, FAF, TUE)\n",
    "   - Justification: Features have different scales (Age: years, Height: meters)\n",
    "   - Method: StandardScaler to ensure mean=0, std=1\n",
    "\n",
    "5. Create BMI Feature (derived attribute - from 2a)\n",
    "   - Formula: BMI = Weight / (Height^2)\n",
    "   - Justification: BMI is standard obesity measure, may improve interpretability\n",
    "\n",
    "OPTIONAL CONSIDERATIONS:\n",
    "- Remove highly correlated features: Not needed (max correlation 0.463)\n",
    "- Binning: Could bin Age into age groups for interpretability\n",
    "- Interaction features: Height  Weight interaction\n",
    "- External data: Dietary guidelines, regional health statistics (out of scope)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "engine.insert([\n",
    "    ':plan_data_preparation rdf:type prov:Activity .',\n",
    "    ':plan_data_preparation sc:isPartOf :data_understanding_phase .',\n",
    "    ':plan_data_preparation rdfs:label \"Task 2g: Data Preparation Planning\" .',\n",
    "    f':plan_data_preparation rdfs:comment \"\"\"{t2g_comment}\"\"\" .',\n",
    "\n",
    "    f':plan_data_preparation prov:qualifiedAssociation :{t2g_uuid_exec} .',\n",
    "    f':{t2g_uuid_exec} prov:agent :{t2g_code_writer} .',\n",
    "    f':{t2g_uuid_exec} rdf:type prov:Association .',\n",
    "    f':{t2g_uuid_exec} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # INPUTS - uses all previous reports\n",
    "    ':plan_data_preparation prov:used :statistical_report .',\n",
    "    ':plan_data_preparation prov:used :quality_report .',\n",
    "    ':plan_data_preparation prov:used :ethics_assessment .',\n",
    "    ':plan_data_preparation prov:used :bias_risk_report .',\n",
    "\n",
    "    # OUTPUT\n",
    "    ':preparation_plan rdf:type prov:Entity .',\n",
    "    ':preparation_plan prov:wasGeneratedBy :plan_data_preparation .',\n",
    "    ':preparation_plan rdfs:label \"Data Preparation Action Plan\" .',\n",
    "], prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[26]\u001B[39m\u001B[32m, line 45\u001B[39m\n\u001B[32m      8\u001B[39m t2g_code_writer = student_b\n\u001B[32m      9\u001B[39m t2g_comment = \u001B[33m\"\"\"\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[33mTask 2g: Required Data Preparation Actions\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[33mBased on findings from tasks 2a-2f, the following preparation steps are required:\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     42\u001B[39m \n\u001B[32m     43\u001B[39m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m45\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation rdf:type prov:Activity .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation sc:isPartOf :data_understanding_phase .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation rdfs:label \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mTask 2g: Data Preparation Planning\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation rdfs:comment \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2g_comment\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     50\u001B[39m \n\u001B[32m     51\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation prov:qualifiedAssociation :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2g_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2g_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:agent :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2g_code_writer\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2g_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m rdf:type prov:Association .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mt2g_uuid_exec\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m prov:hadRole :\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcode_writer_role\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     55\u001B[39m \n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# INPUTS - uses all previous reports\u001B[39;49;00m\n\u001B[32m     57\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation prov:used :statistical_report .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation prov:used :quality_report .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation prov:used :ethics_assessment .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:plan_data_preparation prov:used :bias_risk_report .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \n\u001B[32m     62\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# OUTPUT\u001B[39;49;00m\n\u001B[32m     63\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:preparation_plan rdf:type prov:Entity .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:preparation_plan prov:wasGeneratedBy :plan_data_preparation .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m:preparation_plan rdfs:label \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mData Preparation Action Plan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m .\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:940\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    924\u001B[39m         response = urlopener(request, timeout=\u001B[38;5;28mself\u001B[39m.timeout)\n\u001B[32m    925\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m         response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m urllib.error.HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    187\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    493\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m processor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.process_response.get(protocol, []):\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    601\u001B[39m \u001B[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001B[39;00m\n\u001B[32m    602\u001B[39m \u001B[38;5;66;03m# request was successfully received, understood, and accepted.\u001B[39;00m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    531\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_err:\n\u001B[32m    532\u001B[39m     args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    464\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[32m    465\u001B[39m     func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    468\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "382ff5f3e009cb56",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b828c0011fffefb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:10:49.658359Z",
     "start_time": "2026-01-18T16:10:47.429351Z"
    }
   },
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "f':data_preparation_phase rdf:type prov:Activity .',\n",
    "f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .', \n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m## Each Activity that follows is part of the Data Preparation Phase\u001B[39;00m\n\u001B[32m      3\u001B[39m data_preparation_phase_executor = [\n\u001B[32m      4\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:data_preparation_phase rdf:type prov:Activity .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      5\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:data_preparation_phase rdfs:label \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mData Preparation Phase\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m, \n\u001B[32m      6\u001B[39m ]\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_preparation_phase_executor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "c80c76b4b16069aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:11:05.279752Z",
     "start_time": "2026-01-18T16:10:54.487520Z"
    }
   },
   "source": [
    "# ##########################################\n",
    "# 3. DATA PREPARATION (Main Pipeline)\n",
    "# ##########################################\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# functions for data preparation\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes duplicates. Outliers are retained (see 3b).\"\"\"\n",
    "    # 2g: Remove duplicates\n",
    "    df = df.drop_duplicates().copy()\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculates BMI and bins Age.\"\"\"\n",
    "    # 2g: BMI Calculation\n",
    "    df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n",
    "\n",
    "    # 2g: Age Binning (0: Youth, 1: YoungAdult, 2: Adult, 3: Senior)\n",
    "    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 40, 60, 100], labels=[0, 1, 2, 3])\n",
    "    df['Age_Group'] = df['Age_Group'].astype(int)\n",
    "    return df\n",
    "\n",
    "def encode_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # Target\n",
    "    le = LabelEncoder()\n",
    "    df['NObeyesdad'] = le.fit_transform(df['NObeyesdad'])\n",
    "\n",
    "    # Ordinal features\n",
    "    ord_map = {'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}\n",
    "    df['CAEC'] = df['CAEC'].map(ord_map)\n",
    "    df['CALC'] = df['CALC'].map(ord_map)\n",
    "\n",
    "    # Binary features\n",
    "    bin_map = {'no': 0, 'yes': 1}\n",
    "    for c in ['FAVC', 'SCC', 'SMOKE', 'family_history_with_overweight']:\n",
    "        df[c] = df[c].map(bin_map)\n",
    "\n",
    "    # Nominal (One-Hot)\n",
    "    df['Gender'] = df['Gender'].map({'Female': 0, 'Male': 1})\n",
    "    df = pd.get_dummies(df, columns=['MTRANS'], prefix='MTRANS', dtype=int)\n",
    "    return df\n",
    "\n",
    "def scale_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = ['Age', 'Height', 'Weight', 'BMI', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "    return df\n",
    "\n",
    "# execution\n",
    "dp_code_writer = student_b\n",
    "dp_code_executor = executed_by\n",
    "\n",
    "start_time_dp = now()\n",
    "\n",
    "df = clean_data(df)\n",
    "df = feature_engineering(df)\n",
    "df = encode_features(df)\n",
    "df = scale_features(df)\n",
    "\n",
    "end_time_dp = now()\n",
    "print(f\"Data Prep Completed. Final Shape: {df.shape}\")\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# This is the continuation of the example from the Data Understanding phase above.\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => already done in data understanding phase\n",
    "# 2. activity inspects the outcome and derives decisions => already done in data understanding phase\n",
    "# 3. activity follows up on the decision by changing the data => in this case by removing the the outliers that were found\n",
    "\n",
    "ro_ass_uuid_executor = \"ec7e81e1-86ea-475a-a8d4-c7d8ee535000\"\n",
    "\n",
    "dp_executor = [\n",
    "    f':prepare_data prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "try:\n",
    "    engine.insert(dp_executor, prefixes=prefixes)\n",
    "except:\n",
    "    print(\"Graph Error (Executor)\")\n",
    "\n",
    "# Activity & Report Node (Template UUID)\n",
    "td_ass_uuid_writer = \"1405f15a-3545-4014-a962-637f3c10a000\"\n",
    "\n",
    "td_comment = \"\"\"\n",
    "**3a. Applied Pre-processing Actions:**\n",
    "1. **Cleaning:** Deduplicated dataset (24 duplicates removed).\n",
    "2. **Feature Engineering:** Calculated 'BMI' and created 'Age_Group' bins.\n",
    "3. **Encoding:** Applied LabelEncoding (Target), OrdinalEncoding (CAEC/CALC), and OneHotEncoding (MTRANS).\n",
    "4. **Scaling:** Standardized all continuous features (Mean=0, Std=1) to ensure equal model weighting.\n",
    "\"\"\"\n",
    "\n",
    "dp_activity = [\n",
    "    ':prepare_data rdf:type prov:Activity .',\n",
    "    ':prepare_data sc:isPartOf :data_preparation_phase .',\n",
    "    ':prepare_data rdfs:label \"Data Preparation (Full Pipeline)\" .',\n",
    "    f':prepare_data rdfs:comment \"\"\"{td_comment}\"\"\" .',\n",
    "    f':prepare_data prov:startedAtTime \"{start_time_dp}\"^^xsd:dateTime .',\n",
    "    f':prepare_data prov:endedAtTime \"{end_time_dp}\"^^xsd:dateTime .',\n",
    "\n",
    "    f':prepare_data prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{dp_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    ':prepare_data prov:used :data .',\n",
    "    ':prepare_data prov:used :preparation_plan .',\n",
    "\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data prov:wasGeneratedBy :prepare_data .',\n",
    "    ':prepared_data prov:wasDerivedFrom :data .',\n",
    "    ':prepared_data rdf:type sc:Dataset .'\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(dp_activity, prefixes=prefixes)\n",
    "    print(\"Graph: Main Data Prep logged.\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph Error: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prep Completed. Final Shape: (2087, 23)\n",
      "Graph Error (Executor)\n",
      "Graph Error: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure'\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "818c84864207b4a8",
   "metadata": {},
   "source": [
    "**Continue with other tasks of the Data Preparation phase such as binning, scaling etc...**"
   ]
  },
  {
   "cell_type": "code",
   "id": "c71fa5b3436e0e57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:11:15.325716Z",
     "start_time": "2026-01-18T16:11:15.295531Z"
    }
   },
   "source": [
    "#############################################\n",
    "# Documentation 3b: Steps not applied\n",
    "#############################################\n",
    "\n",
    "# report for data inspection\n",
    "comment_3b = \"\"\"\n",
    "Report on Pre-processing Steps Considered but Rejected:\n",
    "\n",
    "During our Data Preparation phase, we evaluated several common techniques but decided not to apply them for the following reasons:\n",
    "\n",
    "1.  Outlier Removal (Statistical):\n",
    "    * Observation: We found many data points in 'Weight' and 'BMI' that are statistically considered outliers (far from the mean).\n",
    "    * Decision: We kept them (RETAINED).\n",
    "    * Reason: Our specific goal is to classify obesity types. The \"outliers\" are actually the people with Obesity Type III. If we remove them to \"clean\" the data, we would delete the exact class we are trying to predict. In this medical context, extreme values are real data, not noise.\n",
    "\n",
    "2.  Imputation of Missing Values:\n",
    "    * Observation: We ran df.isnull().sum() and checked the dataset documentation.\n",
    "    * Result: The dataset is completely full (0 null values).\n",
    "    * Decision: No imputation (filling gaps with mean/median) was necessary because the data quality is already perfect in this regard.\n",
    "\n",
    "3.  Dimensionality Reduction (PCA):\n",
    "    * Idea: We considered using Principal Component Analysis (PCA) to combine features and make the model faster.\n",
    "    * Decision: SKIPPED.\n",
    "    * Reason: Our business goal includes \"Interpretability\". We need to tell users why they are classified as obese (e.g., \"because you don't eat vegetables\"). PCA turns features into abstract math (Component 1, Component 2) which cannot be explained to a doctor or patient.\n",
    "\"\"\"\n",
    "\n",
    "# UUID for the writer of this specific documentation\n",
    "uuid_3b_writer = \"52c3f822-002e-4dff-b2bb-bd1feb076035\"\n",
    "\n",
    "doc_3b_triples = [\n",
    "        ':document_rejected_steps rdf:type prov:Activity .',\n",
    "        ':document_rejected_steps sc:isPartOf :data_preparation_phase .',\n",
    "        ':document_rejected_steps rdfs:label \"Task 3b: Document Rejected Steps\" .',\n",
    "\n",
    "        f':document_rejected_steps prov:qualifiedAssociation :{uuid_3b_writer} .',\n",
    "        f':{uuid_3b_writer} prov:agent :{dp_code_writer} .',\n",
    "        f':{uuid_3b_writer} rdf:type prov:Association .',\n",
    "        f':{uuid_3b_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "        ':data_prep_not_applied rdf:type prov:Entity .',\n",
    "        ':data_prep_not_applied prov:wasGeneratedBy :document_rejected_steps .',\n",
    "        ':data_prep_not_applied rdfs:label \"3b Steps considered but not applied\" .',\n",
    "        f':data_prep_not_applied rdfs:comment \"\"\"{comment_3b}\"\"\" .',\n",
    "\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(doc_3b_triples, prefixes=prefixes)\n",
    "    print(\"Graph update: 3b (Detailed Report with UUID) logged.\")\n",
    "except Exception as e:\n",
    "    print(f\"Server error: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server error: HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "200576fafe0c471d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:11:24.186315Z",
     "start_time": "2026-01-18T16:11:18.499036Z"
    }
   },
   "source": [
    "#############################################\n",
    "# Documentation 3b: Steps not applied\n",
    "#############################################\n",
    "\n",
    "comment_3b = \"\"\"\n",
    "Report on Pre-processing Steps Considered but Rejected:\n",
    "\n",
    "During our Data Preparation phase, we evaluated several common techniques but decided not to apply them for the following reasons:\n",
    "\n",
    "1.  Outlier Removal (Statistical):\n",
    "    * Observation: We found many data points in 'Weight' and 'BMI' that are statistically considered outliers (far from the mean).\n",
    "    * Decision: We kept them (RETAINED).\n",
    "    * Reason: Our specific goal is to classify obesity types. The \"outliers\" are actually the people with Obesity Type III. If we remove them to \"clean\" the data, we would delete the exact class we are trying to predict. In this medical context, extreme values are real data, not noise.\n",
    "\n",
    "2.  Imputation of Missing Values:\n",
    "    * Observation: We ran df.isnull().sum() and checked the dataset documentation.\n",
    "    * Result: The dataset is completely full (0 null values).\n",
    "    * Decision: No imputation (filling gaps with mean/median) was necessary because the data quality is already perfect in this regard.\n",
    "\n",
    "3.  Dimensionality Reduction (PCA):\n",
    "    * Idea: We considered using Principal Component Analysis (PCA) to combine features and make the model faster.\n",
    "    * Decision: SKIPPED.\n",
    "    * Reason: Our business goal includes \"Interpretability\". We need to tell users why they are classified as obese (e.g., \"because you don't eat vegetables\"). PCA turns features into abstract math (Component 1, Component 2) which cannot be explained to a doctor or patient.\n",
    "\"\"\"\n",
    "\n",
    "# UUID for the writer\n",
    "uuid_3b_writer = \"feee33de-d60c-4f0a-934b-628d946a1256\"\n",
    "\n",
    "# Check if exists first\n",
    "try:\n",
    "    check_3b = f\"\"\"\n",
    "    {prefix_header}\n",
    "    SELECT ?type WHERE {{\n",
    "        :data_prep_not_applied rdf:type ?type .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    result = engine.query(check_3b)\n",
    "\n",
    "    if not result.empty:\n",
    "        print(\"Task 3b already exists - skipping\")\n",
    "        task_3b_exists = True\n",
    "    else:\n",
    "        print(\"Task 3b doesn't exist - safe to insert\")\n",
    "        task_3b_exists = False\n",
    "except Exception as e:\n",
    "    print(f\"Check failed: {e} - assuming doesn't exist\")\n",
    "    task_3b_exists = False\n",
    "\n",
    "# Only insert if doesn't exist\n",
    "if not task_3b_exists:\n",
    "    doc_3b_triples = [\n",
    "        ':document_rejected_steps rdf:type prov:Activity .',\n",
    "        ':document_rejected_steps sc:isPartOf :data_preparation_phase .',\n",
    "        ':document_rejected_steps rdfs:label \"Task 3b: Document Rejected Steps\" .',\n",
    "\n",
    "        f':document_rejected_steps prov:qualifiedAssociation :{uuid_3b_writer} .',\n",
    "        f':{uuid_3b_writer} prov:agent :{dp_code_writer} .',\n",
    "        f':{uuid_3b_writer} rdf:type prov:Association .',\n",
    "        f':{uuid_3b_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "        ':data_prep_not_applied rdf:type prov:Entity .',\n",
    "        ':data_prep_not_applied prov:wasGeneratedBy :document_rejected_steps .',\n",
    "        ':data_prep_not_applied rdfs:label \"3b Steps considered but not applied\" .',\n",
    "        f':data_prep_not_applied rdfs:comment \"\"\"{comment_3b}\"\"\" .',\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        engine.insert(doc_3b_triples, prefixes=prefixes)\n",
    "        print(\"Task 3b logged successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Insert error: {e}\")\n",
    "else:\n",
    "    print(\" Skipping Task 3b insert - already exists\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check failed: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure' - assuming doesn't exist\n",
      "Insert error: HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "c7621332e4718e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:11:43.555067Z",
     "start_time": "2026-01-18T16:11:37.879974Z"
    }
   },
   "source": [
    "# Check if Task 3b already exists (CORRECT VERSION)\n",
    "try:\n",
    "    check_3b = f\"\"\"\n",
    "    {prefix_header}\n",
    "    SELECT ?type WHERE {{\n",
    "        :data_prep_not_applied rdf:type ?type .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    result = engine.query(check_3b)\n",
    "\n",
    "    if not result.empty:\n",
    "        print(\" Task 3b (:data_prep_not_applied) already exists!\")\n",
    "        print(\"Skipping to avoid 403\")\n",
    "        task_3b_exists = True\n",
    "    else:\n",
    "        print(\" Task 3b doesn't exist - safe to insert\")\n",
    "        task_3b_exists = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Check failed: {e}\")\n",
    "    print(\"Assuming doesn't exist - will try to insert\")\n",
    "    task_3b_exists = False\n",
    "\n",
    "# Then use task_3b_exists to decide whether to insert\n",
    "if not task_3b_exists:\n",
    "    # Your insert code here\n",
    "    doc_3b_triples = [...]\n",
    "    try:\n",
    "        engine.insert(doc_3b_triples, prefixes=prefixes)\n",
    "        print(\" Task 3b logged\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "else:\n",
    "    print(\" Skipping Task 3b insert\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Check failed: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure'\n",
      "Assuming doesn't exist - will try to insert\n",
      " Error: 'ellipsis' object is not subscriptable\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "540e30d5b84b7182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:11:51.663460Z",
     "start_time": "2026-01-18T16:11:45.952621Z"
    }
   },
   "source": [
    "#############################################\n",
    "# Documentation 3c: Derived Attributes\n",
    "#############################################\n",
    "\n",
    "# Detailed for Feature Engineering\n",
    "comment_3c = \"\"\"\n",
    "Analysis of Derived Attributes (Feature Engineering):\n",
    "\n",
    "We analyzed which new features could help the model learn better patterns:\n",
    "\n",
    "1.  BMI (Body Mass Index) - [APPLIED]:\n",
    "    * We calculated this using Weight / (Height^2).\n",
    "    * Why: Even though the model has Height and Weight, providing the explicit BMI ratio helps decision trees make cleaner splits. It is historically the strongest predictor for obesity.\n",
    "\n",
    "2.  Age Grouping (Binning) - [APPLIED]:\n",
    "    * We converted the continuous 'Age' into categories (Youth, Adult, Senior).\n",
    "    * Why: Lifestyle habits change with life stages. A 20-year-old and a 50-year-old might have the same weight but very different health risks. This helps the model find those non-linear patterns.\n",
    "\n",
    "3.  \"Sedentary Ratio\" (TUE / FAF) - [REJECTED]:\n",
    "    * Idea: Create a ratio of \"Time on Technology\" divided by \"Physical Activity\" to quantify a sedentary lifestyle.\n",
    "    * Problem: Many participants have FAF = 0 (no activity). This causes division-by-zero errors. Also, combining them might hide the specific impact of just \"sitting too much\" vs \"not moving enough\". We kept them separate.\n",
    "\n",
    "4.  Healthy Diet Score - [REJECTED]:\n",
    "    * Idea: Summing up vegetable intake and water, subtracting junk food.\n",
    "    * Reason: Information Loss. A person who eats lots of veggies AND lots of junk food is different from someone who eats neither. The model needs to see the individual habits to classify correctly.\n",
    "\"\"\"\n",
    "\n",
    "# UUID for the writer of this specific documentation\n",
    "uuid_3c_writer = \"73c922e3-87d2-5c9b-03f2-b2c3d4e5f6g7\"\n",
    "\n",
    "doc_3c_triples = [\n",
    "    # Activity\n",
    "    ':analyze_derived_attributes rdf:type prov:Activity .',\n",
    "    ':analyze_derived_attributes sc:isPartOf :data_preparation_phase .',\n",
    "    ':analyze_derived_attributes rdfs:label \"Task 3c: Derived Attributes Analysis\" .',\n",
    "\n",
    "    # Association with UUID\n",
    "    f':analyze_derived_attributes prov:qualifiedAssociation :{uuid_3c_writer} .',\n",
    "    f':{uuid_3c_writer} prov:agent :{dp_code_writer} .',\n",
    "    f':{uuid_3c_writer} rdf:type prov:Association .',\n",
    "    f':{uuid_3c_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Entity\n",
    "    ':data_prep_derived_attrs rdf:type prov:Entity .',\n",
    "    ':data_prep_derived_attrs prov:wasGeneratedBy :analyze_derived_attributes .',\n",
    "    ':data_prep_derived_attrs rdfs:label \"3c Derived Attributes Analysis\" .',\n",
    "    f':data_prep_derived_attrs rdfs:comment \"\"\"{comment_3c}\"\"\" .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(doc_3c_triples, prefixes=prefixes)\n",
    "    print(\"Graph update: 3c (Detailed Report with UUID) logged.\")\n",
    "except Exception as e:\n",
    "    print(f\"Server error: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server error: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure'\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "aa1630a774d9be4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:11:58.726163Z",
     "start_time": "2026-01-18T16:11:53.103452Z"
    }
   },
   "source": [
    "#############################################\n",
    "# Documentation 3d: External Data\n",
    "#############################################\n",
    "\n",
    "# report for hypothetical data\n",
    "comment_3d = \"\"\"\n",
    "Analysis of Additional External Data Sources:\n",
    "\n",
    "The current dataset is limited to self-reported surveys. We identified three external sources that would significantly improve model quality in a real-world project:\n",
    "\n",
    "1.  Objective Wearable Data (IoT):\n",
    "    * The Problem: People lie on surveys. They overestimate how much they run and underestimate how much they sit.\n",
    "    * The Solution: Integrating data from Fitbits or Apple Health (Step count, Heart Rate Variability, Active Energy Burn).\n",
    "    * Impact: This would replace subjective \"feelings\" about activity with hard facts, drastically reducing noise in the FAF (Physical Activity) feature.\n",
    "\n",
    "2.  Socio-Economic & Location Data:\n",
    "    * The Context: Obesity is often strongly correlated with income and location (access to healthy food).\n",
    "    * The Data: Linking user Zip Codes to average household income or \"Food Desert\" maps.\n",
    "    * Impact: This would help the model understand if someone eats junk food (FAVC) by choice or because fresh produce isn't available in their area.\n",
    "\n",
    "3.  Medical & Genetic History:\n",
    "    * The Missing Link: The dataset assumes weight is 100% lifestyle. It ignores metabolism.\n",
    "    * The Data: Thyroid function tests, hormone levels, or genetic markers.\n",
    "    * Impact: This would identify patients who are obese due to medical conditions, not just diet. Currently, the model might unfairly classify these patients based on their \"average\" diet.\n",
    "\"\"\"\n",
    "\n",
    "# UUID for the writer of this specific documentation\n",
    "uuid_3d_writer = \"83d033f4-98e3-6d0c-14g3-c3d4e5f6g7h8\"\n",
    "\n",
    "doc_3d_triples = [\n",
    "    # Activity\n",
    "    ':analyze_external_data rdf:type prov:Activity .',\n",
    "    ':analyze_external_data sc:isPartOf :data_preparation_phase .',\n",
    "    ':analyze_external_data rdfs:label \"Task 3d: External Data Analysis\" .',\n",
    "\n",
    "    # Association with UUID\n",
    "    f':analyze_external_data prov:qualifiedAssociation :{uuid_3d_writer} .',\n",
    "    f':{uuid_3d_writer} prov:agent :{dp_code_writer} .',\n",
    "    f':{uuid_3d_writer} rdf:type prov:Association .',\n",
    "    f':{uuid_3d_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Entity\n",
    "    ':data_prep_external_data rdf:type prov:Entity .',\n",
    "    ':data_prep_external_data prov:wasGeneratedBy :analyze_external_data .',\n",
    "    ':data_prep_external_data rdfs:label \"3d External Data Analysis\" .',\n",
    "    f':data_prep_external_data rdfs:comment \"\"\"{comment_3d}\"\"\" .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(doc_3d_triples, prefixes=prefixes)\n",
    "    print(\"Graph update: 3d (Detailed Report with UUID) logged.\")\n",
    "except Exception as e:\n",
    "    print(f\"Server error: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server error: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure'\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "b75e36f55a2b952f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:12:06.312442Z",
     "start_time": "2026-01-18T16:12:01.360140Z"
    }
   },
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    ':prepared_data rdfs:label \"Final Prepared Obesity Dataset\" .',\n",
    "    f':prepared_data rdfs:comment \"Final dataset with {len(df)} rows. Includes BMI, age groups and encoded targets.\" .',\n",
    "\n",
    "    # provenance: derived from raw data (2a), generated by prepare_data (3a)\n",
    "    ':prepared_data prov:wasDerivedFrom :data .',\n",
    "    ':prepared_data prov:wasGeneratedBy :prepare_data .',\n",
    "\n",
    "    # 2. structure (croissant recordset)\n",
    "    ':prepared_recordset rdf:type cr:RecordSet .',\n",
    "    ':prepared_recordset sc:name \"Prepared Data Records\" .',\n",
    "    ':prepared_data cr:recordSet :prepared_recordset .',\n",
    "\n",
    "    # 3. describe new features\n",
    "    # bmi with si unit\n",
    "    ':prepared_recordset cr:field :field_bmi .',\n",
    "    ':field_bmi rdf:type cr:Field .',\n",
    "    ':field_bmi sc:name \"BMI\" .',\n",
    "    ':field_bmi sc:description \"Body Mass Index\" .',\n",
    "    ':field_bmi cr:dataType xsd:double .',\n",
    "    ':field_bmi qudt:unit siu:KilogramPerSquareMetre .',\n",
    "\n",
    "    # age group code (binned)\n",
    "    ':prepared_recordset cr:field :field_age_group_code .',\n",
    "    ':field_age_group_code rdf:type cr:Field .',\n",
    "    ':field_age_group_code sc:name \"Age_Group_Code\" .',\n",
    "    ':field_age_group_code sc:description \"0=Youth, 1=YoungAdult, 2=Adult, 3=Senior\" .',\n",
    "    ':field_age_group_code cr:dataType xsd:integer .',\n",
    "\n",
    "    # target encoded\n",
    "    ':prepared_recordset cr:field :field_target_encoded .',\n",
    "    ':field_target_encoded rdf:type cr:Field .',\n",
    "    ':field_target_encoded sc:name \"NObeyesdad\" .',\n",
    "    ':field_target_encoded sc:description \"Target variable encoded (0-6)\" .',\n",
    "    ':field_target_encoded cr:dataType xsd:integer .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(prepared_data_triples, prefixes=prefixes)\n",
    "    print(\"graph update: prepared data documented.\")\n",
    "except Exception as e:\n",
    "    print(f\"server error: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server error: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure'\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "b8d45a77c69107e0",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b80525ea60125c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:12:14.480498Z",
     "start_time": "2026-01-18T16:12:08.598376Z"
    }
   },
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m## Each Activity that follows is part of the Modeling Phase\u001B[39;00m\n\u001B[32m      3\u001B[39m modeling_phase_executor = [\n\u001B[32m      4\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:modeling_phase rdf:type prov:Activity .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      5\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:modeling rdfs:label \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModeling Phase\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m, \n\u001B[32m      6\u001B[39m ]\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodeling_phase_executor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "a6864c3b769d0af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:12:22.896976Z",
     "start_time": "2026-01-18T16:12:17.280307Z"
    }
   },
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# documentation 4a\n",
    "#############################################\n",
    "\n",
    "# we use a fixed unique string for our group to avoid 403 errors\n",
    "dma_ass_uuid_writer = \"gr74-a-algo-selection-unique-id\"\n",
    "\n",
    "# rationale for choosing random forest\n",
    "dma_comment = \"\"\"\n",
    "for the modeling phase, we selected the random forest classifier.\n",
    "our dataset has 7 different obesity levels as target classes, and\n",
    "about 77 percent of the data is synthetic generated by smote.\n",
    "random forest is an ensemble method that is very robust against\n",
    "overfitting on these synthetic patterns. it also handles the\n",
    "outliers we found in the age and ncp attributes much better\n",
    "than linear models. another advantage is that it provides\n",
    "feature importance, which is great for our public health scenario\n",
    "to see which habits have the most impact on obesity.\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:label \"task 4a algorithm selection\" .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "\n",
    "    # linking the activity to person a\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # algorithm and implementation\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"random forest\" .',\n",
    "\n",
    "    f':random_forrest_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_implementation rdfs:label \"scikit-learn randomforestclassifier\" .',\n",
    "    f':random_forrest_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    # defining evaluation measures for classification\n",
    "    f':accuracy_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':accuracy_measure rdfs:label \"accuracy\" .',\n",
    "    f':accuracy_measure rdfs:comment \"percentage of correct obesity class predictions\" .',\n",
    "    f':accuracy_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    f':f1_macro_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':f1_macro_measure rdfs:label \"f1-score macro\" .',\n",
    "    f':f1_macro_measure rdfs:comment \"macro-averaged f1 score for all 7 labels\" .',\n",
    "    f':f1_macro_measure prov:wasGeneratedBy :define_algorithm .'\n",
    "]\n",
    "\n",
    "# pushing the metadata to the graph\n",
    "try:\n",
    "    engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)\n",
    "    print(\"4a logged successfully: random forest selected.\")\n",
    "except:\n",
    "    print(\"error: check if the node already exists or if there is a server issue.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: check if the node already exists or if there is a server issue.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "75dbe74df8d50afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:12:30.985490Z",
     "start_time": "2026-01-18T16:12:25.351035Z"
    }
   },
   "source": [
    "# --- task 4b: hyper-parameter identification ---\n",
    "# person a is responsible for identifying and justifying the parameters\n",
    "\n",
    "# generate a fixed unique uuid for our group to avoid collisions\n",
    "hp_ass_uuid_writer = \"gr74-a-hp-selection-fixed\"\n",
    "\n",
    "# detailed rationale for tuning max_depth\n",
    "# focuses on preventing overfitting on the 77% synthetic data\n",
    "hp_comment = \"\"\"\n",
    "we identified several hyper-parameters for the random forest classifier,\n",
    "including n_estimators, max_depth, and min_samples_split.\n",
    "for our experiments, we select 'max_depth' as the primary parameter for tuning.\n",
    "this choice is justified because it directly controls the complexity\n",
    "of the individual trees. since the dataset contains 77 percent synthetic\n",
    "records generated by smote, there is a high risk of the model learning\n",
    "noise or artificial patterns. tuning max_depth allows us to find the\n",
    "optimal balance between bias and variance and ensures better\n",
    "generalization for real-world obesity screening.\n",
    "\"\"\"\n",
    "\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:label \"task 4b hyper-parameter identification\" .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "\n",
    "    # link to person a with our fixed uuid\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{student_a} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # define n_estimators as a relevant parameter\n",
    "    f':hp_n_estimators rdf:type mls:HyperParameter .',\n",
    "    f':hp_n_estimators rdfs:label \"n_estimators\" .',\n",
    "    f':hp_n_estimators rdfs:comment \"the number of trees in the forest.\" .',\n",
    "    f':random_forrest_implementation mls:hasHyperParameter :hp_n_estimators .',\n",
    "    f':hp_n_estimators prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # define max_depth as our tuning target\n",
    "    f':hp_max_depth rdf:type mls:HyperParameter .',\n",
    "    f':hp_max_depth rdfs:label \"max_depth\" .',\n",
    "    f':hp_max_depth rdfs:comment \"the maximum depth of the tree to control overfitting.\" .',\n",
    "    f':random_forrest_implementation mls:hasHyperParameter :hp_max_depth .',\n",
    "    f':hp_max_depth prov:wasGeneratedBy :identify_hyperparameters .'\n",
    "]\n",
    "\n",
    "# push the definitions to the graph\n",
    "try:\n",
    "    engine.insert(identify_hp_activity, prefixes=prefixes)\n",
    "    print(\"4b logged successfully: hyperparameters identified and justified.\")\n",
    "except:\n",
    "    print(\"error: check if the node already exists or server issues.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: check if the node already exists or server issues.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "31c4c979830932b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:12:38.458190Z",
     "start_time": "2026-01-18T16:12:32.617195Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 4c: split logic\n",
    "# we use 60% train, 20% validation, 20% test\n",
    "def split_data(df: pd.DataFrame):\n",
    "    x = df.drop(columns=['NObeyesdad'])\n",
    "    y = df['NObeyesdad']\n",
    "\n",
    "    # split test set first (20%)\n",
    "    # stratify is key for our 7 obesity classes\n",
    "    x_train_val, x_test, y_train_val, y_test = train_test_split(\n",
    "        x, y, test_size=0.20, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # split remaining 80% into train (60%) and val (20%)\n",
    "    # 0.25 * 0.8 = 0.2\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "# execute split\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split_data(df)\n",
    "\n",
    "#############################################\n",
    "# documentation 4c\n",
    "#############################################\n",
    "\n",
    "# fixed uuid for our group 74\n",
    "split_ass_uuid_writer = \"gr74-a-split-fixed-id\"\n",
    "\n",
    "# rationale for the split method\n",
    "split_comment = \"\"\"\n",
    "we implemented a stratified 60/20/20 split to handle the 7 obesity classes.\n",
    "stratification ensures that the distribution of obesity levels remains\n",
    "consistent across train, validation, and test sets.\n",
    "we used a fixed random seed (42) to ensure reproducibility as\n",
    "required by the assignment.\n",
    "\"\"\"\n",
    "\n",
    "# set path for prepared data from phase 3\n",
    "input_dataset = \":prepared_data\"\n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:label \"task 4c data splitting\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "\n",
    "    # training set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"training set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"contains {len(x_train)} samples\" .',\n",
    "\n",
    "    # validation set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"validation set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"contains {len(x_val)} samples\" .',\n",
    "\n",
    "    # test set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"test set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"contains {len(x_test)} samples\" .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(define_split_activity, prefixes=prefixes)\n",
    "    print(\"split documented. sizes: train\", len(x_train), \"val\", len(x_val), \"test\", len(x_test))\n",
    "except:\n",
    "    print(\"error logging split - probably info already exists\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error logging split - probably info already exists\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "1055fb57b0445ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:12:47.982578Z",
     "start_time": "2026-01-18T16:12:40.740728Z"
    }
   },
   "source": [
    "# --- task 4d, 4e & 4f: training, tuning and selection ---\n",
    "# person a: running the tuning loop and selecting the best model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 4d: define parameter space for tuning\n",
    "# we must document all settings tested, not just defaults\n",
    "depth_options = [3, 5, 10, 15, 20, None]\n",
    "all_run_metadata = []\n",
    "\n",
    "best_val_acc = 0\n",
    "best_depth_val = None\n",
    "\n",
    "# capture timing for the whole activity\n",
    "start_time_tafm = now()\n",
    "\n",
    "for d in depth_options:\n",
    "    d_label = str(d) if d is not None else \"none\"\n",
    "\n",
    "    # this creates the provenance for every failed and successful run\n",
    "    run_id = f\"run_rf_depth_{d_label}\"\n",
    "    model_id = f\"model_rf_depth_{d_label}\"\n",
    "    hp_setting_id = f\"hp_set_depth_{d_label}\"\n",
    "    eval_id = f\"eval_acc_depth_{d_label}\"\n",
    "\n",
    "    # 1. training\n",
    "    clf = RandomForestClassifier(max_depth=d, n_estimators=100, random_state=42)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # 2. evaluation on validation set\n",
    "    val_preds = clf.predict(x_val)\n",
    "    acc = accuracy_score(y_val, val_preds)\n",
    "\n",
    "    if acc > best_val_acc:\n",
    "        best_val_acc = acc\n",
    "        best_depth_val = d\n",
    "\n",
    "    # 3. automate triple generation for this specific run\n",
    "    all_run_metadata.extend([\n",
    "        # parameter setting [cite: 110]\n",
    "        f':{hp_setting_id} rdf:type mls:HyperParameterSetting .',\n",
    "        f':{hp_setting_id} mls:specifiedBy :hp_max_depth .',\n",
    "        f':{hp_setting_id} mls:hasValue \"{d_label}\" .',\n",
    "        f':{hp_setting_id} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "\n",
    "        # the run itself\n",
    "        f':{run_id} rdf:type mls:Run .',\n",
    "        f':{run_id} sc:isPartOf :train_and_finetune_model .',\n",
    "        f':{run_id} mls:realizes :random_forest_algorithm .',\n",
    "        f':{run_id} mls:hasInput :training_set .',\n",
    "        f':{run_id} mls:hasInput :{hp_setting_id} .',\n",
    "        f':{run_id} mls:hasOutput :{model_id} .',\n",
    "        f':{run_id} mls:hasOutput :{eval_id} .',\n",
    "\n",
    "        # the resulting model [cite: 113]\n",
    "        f':{model_id} rdf:type mls:Model .',\n",
    "        f':{model_id} prov:wasGeneratedBy :{run_id} .',\n",
    "        f':{model_id} mlso:trainedOn :training_set .',\n",
    "\n",
    "        # the evaluation result [cite: 111]\n",
    "        f':{eval_id} rdf:type mls:ModelEvaluation .',\n",
    "        f':{eval_id} prov:wasGeneratedBy :{run_id} .',\n",
    "        f':{eval_id} mls:hasValue \"{acc}\"^^xsd:double .',\n",
    "        f':{eval_id} mls:specifiedBy :accuracy_measure .',\n",
    "        f':{eval_id} prov:used :validation_set .'\n",
    "    ])\n",
    "\n",
    "end_time_tafm = now()\n",
    "\n",
    "#############################################\n",
    "# final documentation list\n",
    "#############################################\n",
    "\n",
    "# fixed id for our group\n",
    "tafm_ass_uuid_writer = \"gr74-a-tuning-session-fixed\"\n",
    "\n",
    "# 4f: document the decision for the best model\n",
    "tafm_comment = f\"\"\"\n",
    "we tested max_depth levels from 3 to none.\n",
    "the best performance on the validation set was {best_val_acc:.4f}\n",
    "achieved with max_depth={best_depth_val}.\n",
    "this model is selected for final evaluation as it balances\n",
    "complexity and accuracy effectively.\n",
    "\"\"\"\n",
    "\n",
    "# this list contains the main activity info\n",
    "train_model_activity_main = [\n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:label \"task 4d & 4e training and tuning\" .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{student_a} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "]\n",
    "\n",
    "# join the main activity and all automated runs into one list\n",
    "# this is why the 'variable' looked shorter before, but the content is huge!\n",
    "full_modeling_triples = train_model_activity_main + all_run_metadata\n",
    "\n",
    "try:\n",
    "    engine.insert(full_modeling_triples, prefixes=prefixes)\n",
    "    print(f\"logged all {len(depth_options)} runs. best depth: {best_depth_val}\")\n",
    "except:\n",
    "    print(\"graph error - check for duplicate uris if re-running\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph error - check for duplicate uris if re-running\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "ad15b6aeb6a6aa26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:12:55.667719Z",
     "start_time": "2026-01-18T16:12:49.765420Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- task 4g: final model retraining ---\n",
    "\n",
    "# combine sets for final training\n",
    "x_final_train = pd.concat([x_train, x_val])\n",
    "y_final_train = pd.concat([y_train, y_val])\n",
    "\n",
    "# use the best depth found in 4d\n",
    "final_clf = RandomForestClassifier(\n",
    "    max_depth=best_depth_val,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time_final = now()\n",
    "final_clf.fit(x_final_train, y_final_train)\n",
    "end_time_final = now()\n",
    "\n",
    "#############################################\n",
    "# documentation 4g\n",
    "#############################################\n",
    "\n",
    "# using the provided fixed uuid\n",
    "retrain_ass_uuid_writer = \"96815ee0-524c-437b-b5fa-2e15b945c993\"\n",
    "\n",
    "# simple rationale for retraining\n",
    "final_model_comment = f\"\"\"\n",
    "retrained the final random forest model on the complete\n",
    "training and validation data using max_depth={best_depth_val}.\n",
    "\"\"\"\n",
    "\n",
    "retrain_documentation = [\n",
    "    f':retrain_final_model rdf:type prov:Activity .',\n",
    "    f':retrain_final_model sc:isPartOf :modeling_phase .',\n",
    "    f':retrain_final_model rdfs:label \"task 4g: final retraining\" .',\n",
    "    f':retrain_final_model rdfs:comment \"\"\"{final_model_comment}\"\"\" .',\n",
    "    f':retrain_final_model prov:startedAtTime \"{start_time_final}\"^^xsd:dateTime .',\n",
    "    f':retrain_final_model prov:endedAtTime \"{end_time_final}\"^^xsd:dateTime .',\n",
    "\n",
    "    # link to person a\n",
    "    f':retrain_final_model prov:qualifiedAssociation :{retrain_ass_uuid_writer} .',\n",
    "    f':{retrain_ass_uuid_writer} prov:agent :{student_a} .',\n",
    "    f':{retrain_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{retrain_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # inputs and output\n",
    "    f':retrain_final_model prov:used :training_set .',\n",
    "    f':retrain_final_model prov:used :validation_set .',\n",
    "    f':final_model_entity rdf:type mls:Model .',\n",
    "    f':final_model_entity prov:wasGeneratedBy :retrain_final_model .',\n",
    "    f':final_model_entity mlso:trainedOn :training_set .'\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(retrain_documentation, prefixes=prefixes)\n",
    "    print(\"4g logged: final model created and stored.\")\n",
    "except:\n",
    "    print(\"graph error - check for duplicate nodes.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph error - check for duplicate nodes.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "1cfcdd2f50888bb8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1f64763264df6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:13:03.734916Z",
     "start_time": "2026-01-18T16:12:57.887396Z"
    }
   },
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m## Each Activity that follows is part of the Evaluation Phase\u001B[39;00m\n\u001B[32m      3\u001B[39m evaluation_phase_executor = [\n\u001B[32m      4\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:evaluation_phase rdf:type prov:Activity .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      5\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:evaluation_phase rdfs:label \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEvaluation Phase\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m, \n\u001B[32m      6\u001B[39m ]\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluation_phase_executor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "15b794add91f3a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:13:13.782407Z",
     "start_time": "2026-01-18T16:13:08.172095Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# --- task 5: evaluation ---\n",
    "# person b is responsible for this phase\n",
    "\n",
    "def evaluate_on_test_data(model, x_test, y_test):\n",
    "    # predict on test data\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 5e: bias evaluation (checking gender bias)\n",
    "    test_df = x_test.copy()\n",
    "    test_df['target'] = y_test\n",
    "    test_df['pred'] = y_pred\n",
    "\n",
    "    # gender 0 = female, 1 = male\n",
    "    acc_female = accuracy_score(test_df[test_df['Gender'] <= 0]['target'],\n",
    "                                test_df[test_df['Gender'] <= 0]['pred'])\n",
    "    acc_male = accuracy_score(test_df[test_df['Gender'] > 0]['target'],\n",
    "                              test_df[test_df['Gender'] > 0]['pred'])\n",
    "\n",
    "    bias_report = f\"accuracy female: {acc_female:.4f}, accuracy male: {acc_male:.4f}\"\n",
    "    return acc, bias_report\n",
    "\n",
    "eval_code_writer = student_b\n",
    "start_time_eval = now()\n",
    "# using final_clf from task 4g\n",
    "test_performance, gender_bias_results = evaluate_on_test_data(final_clf, x_test, y_test)\n",
    "end_time_eval = now()\n",
    "\n",
    "#############################################\n",
    "# documentation\n",
    "#############################################\n",
    "\n",
    "# changed to a fixed unique id for group 74\n",
    "eval_ass_uuid = \"gr74-b-eval-final-fixed\"\n",
    "final_model = \":final_model_entity\"\n",
    "test_set = \":test_set\"\n",
    "\n",
    "eval_comment = f\"\"\"\n",
    "the final random forest model achieved a test accuracy of {test_performance:.4f}.\n",
    "this meets our data mining success criteria.\n",
    "we compared it against kaggle benchmarks (95-99%) and a random baseline.\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluation_phase rdf:type prov:Activity .',\n",
    "    f':evaluation_phase rdfs:label \"evaluation phase\" .',\n",
    "\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"final model evaluation on test set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "\n",
    "    # link to person b with our new fixed id\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "\n",
    "    # metrics\n",
    "    f':test_performance_result rdf:type mls:ModelEvaluation .',\n",
    "    f':test_performance_result mls:hasValue \"{test_performance}\"^^xsd:double .',\n",
    "    f':test_performance_result mls:specifiedBy :accuracy_measure .',\n",
    "    f':test_performance_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "\n",
    "    # 5e: bias analysis\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"bias analysis (gender)\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"{gender_bias_results}\" .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(evaluate_activity, prefixes=prefixes)\n",
    "    print(f\"evaluation logged with fixed id. accuracy: {test_performance:.4f}\")\n",
    "except:\n",
    "    print(\"error - check if uris already exist\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error - check if uris already exist\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:14:07.450115Z",
     "start_time": "2026-01-18T16:14:01.854469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#############################################\n",
    "# TASK 5b: BASELINE AND SOTA PERFORMANCE\n",
    "#############################################\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n=== Task 5b: Baseline and SOTA Evaluation ===\\n\")\n",
    "\n",
    "# Calculate baselines\n",
    "random_clf = DummyClassifier(strategy='uniform', random_state=42)\n",
    "random_clf.fit(x_train, y_train)\n",
    "random_acc = accuracy_score(y_test, random_clf.predict(x_test))\n",
    "\n",
    "majority_clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "majority_clf.fit(x_train, y_train)\n",
    "majority_acc = accuracy_score(y_test, majority_clf.predict(x_test))\n",
    "\n",
    "print(f\"Random Baseline:   {random_acc:.4f}\")\n",
    "print(f\"Majority Baseline: {majority_acc:.4f}\")\n",
    "print(f\"Our Model:         {test_performance:.4f}\")\n",
    "print(f\"Improvement:       {(test_performance - random_acc)*100:.1f} percentage points\")\n",
    "print()\n",
    "\n",
    "# SOTA documentation\n",
    "sota_comment = \"\"\"\n",
    "State-of-the-Art Research:\n",
    "- Palechor 2019 (original paper): 97.14% accuracy with MLP, 95.71% with Decision Trees\n",
    "- Kaggle competitions: 95-99% accuracy range\n",
    "- Our model targets >=90% as per success criteria\n",
    "Source: Palechor & De la Hoz Manotas (2019), Data in Brief, Vol 25\n",
    "\"\"\"\n",
    "print(\"SOTA Benchmark: Palechor 2019 achieved 97.14% (MLP)\")\n",
    "print()\n",
    "\n",
    "# Provenance\n",
    "baseline_ass_uuid = \"gr74-b-baseline-eval-fixed\"\n",
    "baseline_comment = f\"Baseline evaluation: Random={random_acc:.4f}, Majority={majority_acc:.4f}, Our model={test_performance:.4f}. Model exceeds random baseline by {(test_performance - random_acc)*100:.1f} percentage points. SOTA from literature: 97.14% (Palechor 2019).\"\n",
    "\n",
    "baseline_activity = [\n",
    "    f':evaluate_baselines rdf:type prov:Activity .',\n",
    "    f':evaluate_baselines sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_baselines rdfs:label \"Task 5b: Baseline and SOTA Evaluation\" .',\n",
    "    f':evaluate_baselines rdfs:comment \"{baseline_comment}\" .',\n",
    "    f':evaluate_baselines prov:qualifiedAssociation :{baseline_ass_uuid} .',\n",
    "    f':{baseline_ass_uuid} prov:agent :{student_b} .',\n",
    "    f':{baseline_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{baseline_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "    f':random_baseline_result rdf:type mls:ModelEvaluation .',\n",
    "    f':random_baseline_result mls:hasValue \"{random_acc}\"^^xsd:double .',\n",
    "    f':random_baseline_result rdfs:label \"Random Baseline\" .',\n",
    "    f':random_baseline_result prov:wasGeneratedBy :evaluate_baselines .',\n",
    "    f':majority_baseline_result rdf:type mls:ModelEvaluation .',\n",
    "    f':majority_baseline_result mls:hasValue \"{majority_acc}\"^^xsd:double .',\n",
    "    f':majority_baseline_result rdfs:label \"Majority Baseline\" .',\n",
    "    f':majority_baseline_result prov:wasGeneratedBy :evaluate_baselines .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(baseline_activity, prefixes=prefixes)\n",
    "    print(\"Task 5b logged to GraphDB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ],
   "id": "5e8cd581a552eb15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Task 5b: Baseline and SOTA Evaluation ===\n",
      "\n",
      "Random Baseline:   0.1722\n",
      "Majority Baseline: 0.1675\n",
      "Our Model:         0.9856\n",
      "Improvement:       81.3 percentage points\n",
      "\n",
      "SOTA Benchmark: Palechor 2019 achieved 97.14% (MLP)\n",
      "\n",
      "Error: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure'\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:15:40.427595Z",
     "start_time": "2026-01-18T16:15:34.456542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#############################################\n",
    "# TASK 5c: DETAILED PERFORMANCE COMPARISON\n",
    "#############################################\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(\"\\n=== Task 5c: Detailed Performance Comparison ===\\n\")\n",
    "\n",
    "y_pred = final_clf.predict(x_test)\n",
    "class_names = ['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I',\n",
    "               'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']\n",
    "\n",
    "# 1. Confusion Matrix (simplified plot)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        plt.text(j, i, str(cm[i,j]), ha='center', va='center')\n",
    "plt.tight_layout()\n",
    "os.makedirs('data/figures', exist_ok=True)\n",
    "plt.savefig('data/figures/confusion_matrix.png', dpi=150)\n",
    "print(\"Confusion matrix saved\")\n",
    "plt.close()\n",
    "\n",
    "# 2. Per-class metrics (text only, no plots)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# 3. Calculate aggregate metrics\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\nMacro F1:        {macro_f1:.4f}\")\n",
    "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:    {macro_recall:.4f}\")\n",
    "print()\n",
    "\n",
    "# Provenance\n",
    "detailed_comp_uuid = \"gr74-b-detailed-comparison-fixed\"\n",
    "detailed_comp_comment = f\"Confusion matrix analysis completed. Macro metrics: F1={macro_f1:.4f}, Precision={macro_precision:.4f}, Recall={macro_recall:.4f}. Model shows balanced performance across all 7 obesity classes.\"\n",
    "\n",
    "detailed_comp_activity = [\n",
    "    f':detailed_performance_comparison rdf:type prov:Activity .',\n",
    "    f':detailed_performance_comparison sc:isPartOf :evaluation_phase .',\n",
    "    f':detailed_performance_comparison rdfs:label \"Task 5c: Detailed Performance Comparison\" .',\n",
    "    f':detailed_performance_comparison rdfs:comment \"{detailed_comp_comment}\" .',\n",
    "    f':detailed_performance_comparison prov:qualifiedAssociation :{detailed_comp_uuid} .',\n",
    "    f':{detailed_comp_uuid} prov:agent :{student_b} .',\n",
    "    f':{detailed_comp_uuid} rdf:type prov:Association .',\n",
    "    f':{detailed_comp_uuid} prov:hadRole :{code_writer_role} .',\n",
    "    f':confusion_matrix_analysis rdf:type prov:Entity .',\n",
    "    f':confusion_matrix_analysis prov:wasGeneratedBy :detailed_performance_comparison .',\n",
    "    f':confusion_matrix_analysis rdfs:label \"Confusion Matrix\" .',\n",
    "    f':macro_f1_result rdf:type mls:ModelEvaluation .',\n",
    "    f':macro_f1_result mls:hasValue \"{macro_f1}\"^^xsd:double .',\n",
    "    f':macro_f1_result rdfs:label \"Macro F1-score\" .',\n",
    "    f':macro_f1_result prov:wasGeneratedBy :detailed_performance_comparison .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(detailed_comp_activity, prefixes=prefixes)\n",
    "    print(\"Task 5c logged to GraphDB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ],
   "id": "f804e7dd113e1964",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Task 5c: Detailed Performance Comparison ===\n",
      "\n",
      "Confusion matrix saved\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       1.00      1.00      1.00        53\n",
      "      Normal_Weight       0.95      1.00      0.97        57\n",
      " Overweight_Level_I       1.00      1.00      1.00        70\n",
      "Overweight_Level_II       0.98      1.00      0.99        60\n",
      "     Obesity_Type_I       1.00      0.98      0.99        65\n",
      "    Obesity_Type_II       1.00      0.95      0.97        55\n",
      "   Obesity_Type_III       0.97      0.97      0.97        58\n",
      "\n",
      "           accuracy                           0.99       418\n",
      "          macro avg       0.99      0.99      0.99       418\n",
      "       weighted avg       0.99      0.99      0.99       418\n",
      "\n",
      "\n",
      "Macro F1:        0.9851\n",
      "Macro Precision: 0.9856\n",
      "Macro Recall:    0.9851\n",
      "\n",
      "Error: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure'\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:16:00.292722Z",
     "start_time": "2026-01-18T16:15:54.627552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#############################################\n",
    "# TASK 5d: COMPARE WITH BUSINESS SUCCESS CRITERIA (SIMPLIFIED)\n",
    "#############################################\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n=== Task 5d: Success Criteria Comparison ===\\n\")\n",
    "\n",
    "# Recalculate gender bias\n",
    "test_df_bias = x_test.copy()\n",
    "test_df_bias['target'] = y_test.values\n",
    "test_df_bias['pred'] = final_clf.predict(x_test)\n",
    "acc_female = accuracy_score(test_df_bias[test_df_bias['Gender'] <= 0]['target'],\n",
    "                            test_df_bias[test_df_bias['Gender'] <= 0]['pred'])\n",
    "acc_male = accuracy_score(test_df_bias[test_df_bias['Gender'] > 0]['target'],\n",
    "                          test_df_bias[test_df_bias['Gender'] > 0]['pred'])\n",
    "\n",
    "# Check criteria\n",
    "print(\"DATA MINING SUCCESS CRITERIA:\")\n",
    "print(f\"1. Accuracy >=90%: {test_performance:.4f} - {'MET' if test_performance >= 0.90 else 'NOT MET'}\")\n",
    "print(f\"2. Macro F1 >=0.85: {macro_f1:.4f} - {'MET' if macro_f1 >= 0.85 else 'NOT MET'}\")\n",
    "print(f\"3. Baseline improvement >60pp: {(test_performance - random_acc)*100:.1f}pp - {'MET' if (test_performance - random_acc) > 0.60 else 'NOT MET'}\")\n",
    "print()\n",
    "\n",
    "print(\"BUSINESS OBJECTIVES:\")\n",
    "print(f\"1. Early Risk ID: {'ACHIEVED' if test_performance >= 0.90 else 'PARTIAL'}\")\n",
    "print(f\"2. Resource Allocation: {'ACHIEVED' if macro_f1 >= 0.85 else 'PARTIAL'}\")\n",
    "print(f\"3. Interpretability: ACHIEVED (Random Forest)\")\n",
    "print(f\"4. Gender Fairness: Female={acc_female:.4f}, Male={acc_male:.4f}, Gap={abs(acc_female-acc_male):.4f}\")\n",
    "print()\n",
    "\n",
    "# Deployment recommendation\n",
    "if test_performance >= 0.90 and macro_f1 >= 0.85:\n",
    "    recommendation = \"Hybrid deployment with human oversight\"\n",
    "else:\n",
    "    recommendation = \"Limited deployment for screening only\"\n",
    "print(f\"RECOMMENDATION: {recommendation}\")\n",
    "print()\n",
    "\n",
    "# Provenance\n",
    "success_criteria_uuid = \"gr74-b-success-criteria-comparison-fixed\"\n",
    "success_criteria_comment = f\"Success criteria comparison: Accuracy={test_performance:.4f} (target >=0.90), Macro F1={macro_f1:.4f} (target >=0.85). Gender bias: Female={acc_female:.4f}, Male={acc_male:.4f}. Deployment: {recommendation}.\"\n",
    "\n",
    "success_criteria_activity = [\n",
    "    f':compare_success_criteria rdf:type prov:Activity .',\n",
    "    f':compare_success_criteria sc:isPartOf :evaluation_phase .',\n",
    "    f':compare_success_criteria rdfs:label \"Task 5d: Success Criteria Comparison\" .',\n",
    "    f':compare_success_criteria rdfs:comment \"{success_criteria_comment}\" .',\n",
    "    f':compare_success_criteria prov:qualifiedAssociation :{success_criteria_uuid} .',\n",
    "    f':{success_criteria_uuid} prov:agent :{student_b} .',\n",
    "    f':{success_criteria_uuid} rdf:type prov:Association .',\n",
    "    f':{success_criteria_uuid} prov:hadRole :{code_writer_role} .',\n",
    "    f':compare_success_criteria prov:used :bu_business_success_criteria .',\n",
    "    f':compare_success_criteria prov:used :bu_data_mining_success_criteria .',\n",
    "    f':success_criteria_assessment rdf:type prov:Entity .',\n",
    "    f':success_criteria_assessment prov:wasGeneratedBy :compare_success_criteria .',\n",
    "    f':success_criteria_assessment rdfs:label \"Success Criteria Assessment\" .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(success_criteria_activity, prefixes=prefixes)\n",
    "    print(\"Task 5d logged to GraphDB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ],
   "id": "d05b2b81b1782fb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Task 5d: Success Criteria Comparison ===\n",
      "\n",
      "DATA MINING SUCCESS CRITERIA:\n",
      "1. Accuracy >=90%: 0.9856 - MET\n",
      "2. Macro F1 >=0.85: 0.9851 - MET\n",
      "3. Baseline improvement >60pp: 81.3pp - MET\n",
      "\n",
      "BUSINESS OBJECTIVES:\n",
      "1. Early Risk ID: ACHIEVED\n",
      "2. Resource Allocation: ACHIEVED\n",
      "3. Interpretability: ACHIEVED (Random Forest)\n",
      "4. Gender Fairness: Female=0.9798, Male=0.9909, Gap=0.0111\n",
      "\n",
      "RECOMMENDATION: Hybrid deployment with human oversight\n",
      "\n",
      "Error: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'Entity pool initialization failure'\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "fddcc45fc56d8f19",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce9bead770d20d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:18:37.985690Z",
     "start_time": "2026-01-18T16:18:32.109520Z"
    }
   },
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[47]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m## Each Activity that follows is part of the Deployment Phase\u001B[39;00m\n\u001B[32m      3\u001B[39m deployment_phase_executor = [\n\u001B[32m      4\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:deployment_phase rdf:type prov:Activity .\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      5\u001B[39m \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:deployment_phase rdfs:label \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDeployment Phase\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m .\u001B[39m\u001B[33m'\u001B[39m, \n\u001B[32m      6\u001B[39m ]\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeployment_phase_executor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:512\u001B[39m, in \u001B[36mTripleStoreEngine.insert\u001B[39m\u001B[34m(self, triples, prefixes, timestamp, chunk_size)\u001B[39m\n\u001B[32m    510\u001B[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001B[33m\"\u001B[39m\u001B[33mNOW()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    511\u001B[39m     \u001B[38;5;28mself\u001B[39m.sparql_post.setQuery(insert_statement)\n\u001B[32m--> \u001B[39m\u001B[32m512\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    513\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mTriples inserted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "7c42bf712305297e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:18:52.363175Z",
     "start_time": "2026-01-18T16:18:41.061560Z"
    }
   },
   "source": [
    "#############################################\n",
    "# documentation phase 6: deployment\n",
    "#############################################\n",
    "\n",
    "# 6a: reflection on business objectives and success criteria\n",
    "# compare performance to the goals from phase 1\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "our final random forest model achieved a test accuracy of over 90 percent,\n",
    "which successfully fulfills the business success criteria defined during\n",
    "the first phase of crisp-dm. since the performance is consistently high\n",
    "across all seven obesity levels, the tool provides a solid foundation for\n",
    "public health agencies to identify at-risk populations early. we recommend\n",
    "a hybrid deployment strategy where the model acts as a preliminary\n",
    "screening tool for clinics in mexico, peru, and colombia. however,\n",
    "automated results must always be verified by healthcare professionals to\n",
    "avoid misdiagnosis, especially for minority classes or unusual lifestyle patterns.\n",
    "\"\"\"\n",
    "\n",
    "# 6b: ethical aspects and risks identified for deployment\n",
    "# mention the smote data and geographic limitations\n",
    "ethical_aspects_comment = \"\"\"\n",
    "the most significant ethical concern is the use of 77 percent synthetic\n",
    "data generated via smote. while this helps with class balance, it might\n",
    "introduce artificial patterns that do not perfectly reflect the biological\n",
    "diversity of real patients. furthermore, the model is geographically\n",
    "limited to three latin american countries, which could lead to bias if\n",
    "applied to other regions with different dietary cultures. we must also\n",
    "ensure that the classification does not lead to patient stigmatization\n",
    "or discrimination in insurance contexts, requiring strict data\n",
    "privacy protocols and human oversight.\n",
    "\"\"\"\n",
    "\n",
    "# 6c: monitoring plan during deployment\n",
    "# define triggers for intervention\n",
    "monitoring_plan_comment = \"\"\"\n",
    "to maintain model reliability, we propose a two-tier monitoring plan.\n",
    "first, we must monitor for data drift, specifically tracking if the\n",
    "distribution of eating habits or transportation modes in new patients\n",
    "shifts significantly from our training set. second, we define a\n",
    "performance trigger: we will perform regular audits against manual medical\n",
    "labels. if the classification accuracy for any specific subgroup or\n",
    "the overall population drops below 85 percent, it will trigger an\n",
    "immediate review and potential retraining of the model.\n",
    "\"\"\"\n",
    "\n",
    "# 6d: reflection on reproducibility\n",
    "# how easy is it to replicate our results?\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "reproducibility of our experiment is high because we used fixed random\n",
    "seeds (42) for all data splits and training runs. every processing step,\n",
    "from initial data cleaning and bmi calculation to the final hyperparameter\n",
    "tuning of the random forest, is documented within this provenance\n",
    "knowledge graph. a remaining risk for reproducibility lies in the\n",
    "dependency on specific library versions for scaling and preprocessing,\n",
    "which must be documented clearly in the final report to ensure\n",
    "consistent results across different environments.\n",
    "\"\"\"\n",
    "\n",
    "# fixed unique id for our group 74\n",
    "dep_ass_uuid_executor = \"gr74-ab-deployment-final-fixed\"\n",
    "\n",
    "deployment_executor = [\n",
    "    f':deployment_phase rdf:type prov:Activity .',\n",
    "    f':deployment_phase rdfs:label \"deployment phase\" .',\n",
    "\n",
    "    f':plan_deployment rdf:type prov:Activity .',\n",
    "    f':plan_deployment sc:isPartOf :deployment_phase .',\n",
    "    f':plan_deployment rdfs:label \"plan deployment\" .',\n",
    "\n",
    "    f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "    f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(deployment_executor, prefixes=prefixes)\n",
    "    print(\"deployment activity logged\")\n",
    "except:\n",
    "    print(\"activity already exists - check uris\")\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "    # 6a\n",
    "    f':dep_recommendations rdf:type prov:Entity .',\n",
    "    f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_recommendations rdfs:label \"6a business objectives reflection\" .',\n",
    "    f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "    # 6b\n",
    "    f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "    f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_ethical_risks rdfs:label \"6b ethical aspects and risks\" .',\n",
    "    f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "    # 6c\n",
    "    f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "    f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_monitoring_plan rdfs:label \"6c monitoring plan\" .',\n",
    "    f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "    # 6d\n",
    "    f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "    f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_reproducibility_reflection rdfs:label \"6d reproducibility reflection\" .',\n",
    "    f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(deployment_data_executor, prefixes=prefixes)\n",
    "    print(\"deployment data logged successfully\")\n",
    "except:\n",
    "    print(\"entities already exist in graph\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity already exists - check uris\n",
      "entities already exist in graph\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "973437a77bd888d5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "79c747c8bcf7bd17",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4da8f75831f0",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "id": "5934ea5983bcaaad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:18:52.426561Z",
     "start_time": "2026-01-18T16:18:52.424250Z"
    }
   },
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "596d2ea0c8b41be8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:18:57.793441Z",
     "start_time": "2026-01-18T16:18:57.788666Z"
    }
   },
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "5314307feb7b819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:19:12.907529Z",
     "start_time": "2026-01-18T16:19:10.050991Z"
    }
   },
   "source": [
    "# ++++++++++++++++++++++++++++++++++ FINAL QUERIES (Phases 1-6) +++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# 1. Authors & Business Understanding (Sections 1.1 - 1.4)\n",
    "author_query = f\"\"\"{prefix_header} PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  ?uri a foaf:Person ; foaf:givenName ?given ; foaf:familyName ?family ; iao:IAO_0000219 ?matr .\n",
    "}}\"\"\"\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "if not res_authors.empty:\n",
    "    for _, row in res_authors.iterrows():\n",
    "        given, family, matr = [latex_escape(clean_rdf(row[k])) for k in ['given', 'family', 'matr']]\n",
    "        resp = \"Student A\" if student_a in str(row['uri']) else \"Student B\"\n",
    "        author_block_latex += rf\"\\author{{{given} {family}}} \\authornote{{{resp}, Matr.Nr.: {matr}}} \\affiliation{{\\institution{{TU Wien}} \\country{{Austria}}}}\"\n",
    "\n",
    "bu_query = f\"{prefix_header} SELECT ?ds ?bo ?bsc ?dmg WHERE {{ OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds . }} OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo . }} OPTIONAL {{ :bu_business_success_criteria rdfs:comment ?bsc . }} OPTIONAL {{ :bu_data_mining_goals rdfs:comment ?dmg . }} }} LIMIT 1\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {}\n",
    "bu_data_source, bu_objectives, bu_success_crit, bu_mining_goals = [latex_escape(clean_rdf(row_bu.get(k, \"\"))) for k in [\"ds\", \"bo\", \"bsc\", \"dmg\"]]\n",
    "\n",
    "# 2. Data Understanding (Section 2.1 - 2.7)\n",
    "du_desc_query = f\"{prefix_header} SELECT ?desc WHERE {{ :data sc:description ?desc . }} LIMIT 1\"\n",
    "du_description = latex_escape(clean_rdf(engine.query(du_desc_query).iloc[0].get(\"desc\", \"\"))) if not engine.query(du_desc_query).empty else \"\"\n",
    "\n",
    "# 2a Table\n",
    "du_fields_query = f\"{prefix_header} SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{ :data cr:recordSet ?rs . ?rs cr:field ?field . ?field sc:name ?name ; sc:description ?descRaw ; cr:dataType ?dtypeRaw . }} GROUP BY ?name ORDER BY ?name\"\n",
    "res_du = engine.query(du_fields_query)\n",
    "du_table_rows = \"\\n    \".join([f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(clean_rdf(f['dtype']).split('#')[-1])} & {latex_escape(clean_rdf(f['desc']))} \\\\\\\\\" for _, f in res_du.iterrows()]) if not res_du.empty else \"\"\n",
    "\n",
    "# 2b-2g Summaries\n",
    "def get_comm(uri): return latex_escape(clean_rdf(engine.query(f\"{prefix_header} SELECT ?c WHERE {{ {uri} rdfs:comment ?c . }} LIMIT 1\").iloc[0].get(\"c\", \"\"))) if not engine.query(f\"{prefix_header} SELECT ?c WHERE {{ {uri} rdfs:comment ?c . }} LIMIT 1\").empty else \"\"\n",
    "du_statistics_summary = get_comm(\":analyze_statistics\")\n",
    "du_quality_summary = get_comm(\":assess_data_quality\")\n",
    "du_ethics_summary = get_comm(\":assess_ethical_sensitivity\")\n",
    "du_bias_summary = get_comm(\":analyze_bias_risks\")\n",
    "du_prep_plan = get_comm(\":plan_data_preparation\")\n",
    "\n",
    "# 2d - Visual Exploration Path\n",
    "res_du_viz = engine.query(f\"{prefix_header} SELECT ?comment ?url WHERE {{ :explore_visually rdfs:comment ?comment . OPTIONAL {{ :visualization_report sc:contentUrl ?url . }} }} LIMIT 1\")\n",
    "row_du_viz = res_du_viz.iloc[0] if not res_du_viz.empty else {}\n",
    "du_viz_summary = latex_escape(clean_rdf(row_du_viz.get(\"comment\", \"\")))\n",
    "du_viz_path = clean_rdf(row_du_viz.get(\"url\", \"\")).replace(\"file://\", \"\")\n",
    "\n",
    "# 3. Data Preparation (Section 3.1 - 3.4)\n",
    "dp_res = engine.query(f\"{prefix_header} SELECT ?p ?r ?d ?e WHERE {{ OPTIONAL {{ :prepare_data rdfs:comment ?p . }} OPTIONAL {{ :data_prep_not_applied rdfs:comment ?r . }} OPTIONAL {{ :data_prep_derived_attrs rdfs:comment ?d . }} OPTIONAL {{ :data_prep_external_data rdfs:comment ?e . }} }} LIMIT 1\")\n",
    "row_dp = dp_res.iloc[0] if not dp_res.empty else {}\n",
    "dp_summary, dp_rejected, dp_derived, dp_external = [latex_escape(clean_rdf(row_dp.get(k, \"\"))) for k in [\"p\", \"r\", \"d\", \"e\"]]\n",
    "\n",
    "# 4. Modeling (Algorithm, Split, Tuning-Plot)\n",
    "mod_res = engine.query(f\"{prefix_header} SELECT ?a ?h ?s ?r WHERE {{ OPTIONAL {{ :define_algorithm rdfs:comment ?a . }} OPTIONAL {{ :identify_hyperparameters rdfs:comment ?h . }} OPTIONAL {{ :define_data_split rdfs:comment ?s . }} OPTIONAL {{ :retrain_final_model rdfs:comment ?r . }} }} LIMIT 1\")\n",
    "row_mod = mod_res.iloc[0] if not mod_res.empty else {}\n",
    "mod_algo_text, mod_hp_text, mod_split_text, mod_retrain_text = [latex_escape(clean_rdf(row_mod.get(k, \"\"))) for k in [\"a\", \"h\", \"s\", \"r\"]]\n",
    "\n",
    "# --- NEW: Get Tuning Plot Path (Task 4d) ---\n",
    "res_tuning_plot = engine.query(f\"{prefix_header} SELECT ?url WHERE {{ :tuning_plot_entity sc:contentUrl ?url . }} LIMIT 1\")\n",
    "final_plot_path_clean = clean_rdf(res_tuning_plot.iloc[0].get(\"url\", \"\")).replace(\"file://\", \"\") if not res_tuning_plot.empty else \"\"\n",
    "\n",
    "# 5. Evaluation (Performance & Bias)\n",
    "eval_res = engine.query(f\"{prefix_header} SELECT ?e ?p ?b WHERE {{ OPTIONAL {{ :evaluate_final_model rdfs:comment ?e . }} OPTIONAL {{ :test_performance_result mls:hasValue ?p . }} OPTIONAL {{ :bias_evaluation_result rdfs:comment ?b . }} }} LIMIT 1\")\n",
    "row_eval = eval_res.iloc[0] if not eval_res.empty else {}\n",
    "eval_main_text = latex_escape(clean_rdf(row_eval.get(\"e\", \"\")))\n",
    "eval_perf_val = clean_rdf(row_eval.get(\"p\", \"\"))\n",
    "eval_bias_text = latex_escape(clean_rdf(row_eval.get(\"b\", \"\")))\n",
    "\n",
    "# 5b. Baseline and SOTA Evaluation\n",
    "baseline_res = engine.query(f\"{prefix_header} SELECT ?comm ?rand ?maj WHERE {{ OPTIONAL {{ :evaluate_baselines rdfs:comment ?comm . }} OPTIONAL {{ :random_baseline_result mls:hasValue ?rand . }} OPTIONAL {{ :majority_baseline_result mls:hasValue ?maj . }} }} LIMIT 1\")\n",
    "row_baseline = baseline_res.iloc[0] if not baseline_res.empty else {}\n",
    "eval_baseline_text = latex_escape(clean_rdf(row_baseline.get(\"comm\", \"\")))\n",
    "eval_random_val = clean_rdf(row_baseline.get(\"rand\", \"\"))\n",
    "eval_majority_val = clean_rdf(row_baseline.get(\"maj\", \"\"))\n",
    "\n",
    "# 5c. Detailed Performance Comparison (Confusion Matrix)\n",
    "detailed_res = engine.query(f\"{prefix_header} SELECT ?comm ?f1 WHERE {{ OPTIONAL {{ :detailed_performance_comparison rdfs:comment ?comm . }} OPTIONAL {{ :macro_f1_result mls:hasValue ?f1 . }} }} LIMIT 1\")\n",
    "row_detailed = detailed_res.iloc[0] if not detailed_res.empty else {}\n",
    "eval_detailed_text = latex_escape(clean_rdf(row_detailed.get(\"comm\", \"\")))\n",
    "eval_macro_f1_val = clean_rdf(row_detailed.get(\"f1\", \"\"))\n",
    "\n",
    "# 5d. Success Criteria Comparison\n",
    "success_res = engine.query(f\"{prefix_header} SELECT ?comm WHERE {{ OPTIONAL {{ :compare_success_criteria rdfs:comment ?comm . }} }} LIMIT 1\")\n",
    "row_success = success_res.iloc[0] if not success_res.empty else {}\n",
    "eval_success_text = latex_escape(clean_rdf(row_success.get(\"comm\", \"\")))\n",
    "\n",
    "\n",
    "# 6. Deployment\n",
    "dep_res = engine.query(f\"{prefix_header} SELECT ?r ?e ?m ?p WHERE {{ OPTIONAL {{ :dep_recommendations rdfs:comment ?r . }} OPTIONAL {{ :dep_ethical_risks rdfs:comment ?e . }} OPTIONAL {{ :dep_monitoring_plan rdfs:comment ?m . }} OPTIONAL {{ :dep_reproducibility_reflection rdfs:comment ?p . }} }} LIMIT 1\")\n",
    "row_dep = dep_res.iloc[0] if not dep_res.empty else {}\n",
    "dep_rec, dep_eth, dep_mon, dep_repr = [latex_escape(clean_rdf(row_dep.get(k, \"\"))) for k in [\"r\", \"e\", \"m\", \"p\"]]\n",
    "\n",
    "print(\"Final report queries completed. Paths cleaned and ready for LaTeX.\")"
   ],
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:926\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    925\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m     response = \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m.returnFormat\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:495\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    494\u001B[39m     meth = \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m     response = \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:604\u001B[39m, in \u001B[36mHTTPErrorProcessor.http_response\u001B[39m\u001B[34m(self, request, response)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[32m200\u001B[39m <= code < \u001B[32m300\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhttp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:533\u001B[39m, in \u001B[36mOpenerDirector.error\u001B[39m\u001B[34m(self, proto, *args)\u001B[39m\n\u001B[32m    532\u001B[39m args = (\u001B[38;5;28mdict\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mhttp_error_default\u001B[39m\u001B[33m'\u001B[39m) + orig_args\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    465\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:613\u001B[39m, in \u001B[36mHTTPDefaultErrorHandler.http_error_default\u001B[39m\u001B[34m(self, req, fp, code, msg, hdrs)\u001B[39m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001B[31mHTTPError\u001B[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEndPointInternalError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[52]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# ++++++++++++++++++++++++++++++++++ FINAL QUERIES (Phases 1-6) +++++++++++++++++++++++++++++++++++++++++++++\u001B[39;00m\n\u001B[32m      2\u001B[39m \n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# 1. Authors & Business Understanding (Sections 1.1 - 1.4)\u001B[39;00m\n\u001B[32m      4\u001B[39m author_query = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprefix_header\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m PREFIX iao: <http://purl.obolibrary.org/obo/>\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[33mSELECT DISTINCT ?uri ?given ?family ?matr WHERE \u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[33m  VALUES ?uri \u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m :\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudent_a\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m :\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudent_b\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[33m  ?uri a foaf:Person ; foaf:givenName ?given ; foaf:familyName ?family ; iao:IAO_0000219 ?matr .\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m res_authors = \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mauthor_query\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m author_block_latex = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m res_authors.empty:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/starvers/starvers.py:413\u001B[39m, in \u001B[36mTripleStoreEngine.query\u001B[39m\u001B[34m(self, select_statement, timestamp, yn_timestamp_query, as_df)\u001B[39m\n\u001B[32m    411\u001B[39m \u001B[38;5;66;03m#self.sparql_get_with_post.queryType = 'SELECT'\u001B[39;00m\n\u001B[32m    412\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mRetrieving results ...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m413\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparql_get_with_post\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    415\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThe result has the return type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult._get_responseFormat()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    417\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m as_df:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:960\u001B[39m, in \u001B[36mSPARQLWrapper.query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[33m\"\u001B[39m\u001B[33mQueryResult\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    943\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    Execute the query.\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    958\u001B[39m \u001B[33;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[32m    959\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Downloads/ASSIGNMENT3_BI/.venv1/lib/python3.13/site-packages/SPARQLWrapper/Wrapper.py:938\u001B[39m, in \u001B[36mSPARQLWrapper._query\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    936\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e.read())\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m e.code == \u001B[32m500\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e.read())\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    940\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[31mEndPointInternalError\u001B[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "64d086c8781061f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:19:16.480899Z",
     "start_time": "2026-01-18T16:19:16.442842Z"
    }
   },
   "source": [
    "# ++++++++++++++++ FINAL UPDATED LATEX TEMPLATE +++++++++++++++++++++++++\n",
    "\n",
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence Final Report}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Final Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  this report documents the complete crisp-dm cycle for group {group_id} analyzing obesity levels.\n",
    "  it covers business and data understanding, preparation, modeling using random forest,\n",
    "  extensive evaluation including bias analysis, and deployment recommendations.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{computing methodologies~machine learning}}\n",
    "\\keywords{{crisp-dm, provenance, random forest, obesity classification, bias analysis}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. BUSINESS UNDERSTANDING ---\n",
    "\\section{{Business Understanding}}\n",
    "\\subsection{{Data Source and Scenario}} {bu_data_source}\n",
    "\\subsection{{Business Objectives}} {bu_objectives}\n",
    "\\subsection{{Business Success Criteria}} {bu_success_crit}\n",
    "\\subsection{{Data Mining Goals}} {bu_mining_goals}\n",
    "\n",
    "%% --- 2. DATA UNDERSTANDING ---\n",
    "\\section{{Data Understanding}}\n",
    "\\subsection{{Dataset Overview}} {du_description}\n",
    "\\subsection{{Attribute Analysis}}\n",
    "\\begin{{table*}}[t]\n",
    "  \\caption{{dataset features}}\n",
    "  \\small\n",
    "  \\begin{{tabular}}{{p{{0.18\\linewidth}}p{{0.12\\linewidth}}p{{0.62\\linewidth}}}}\n",
    "    \\toprule \\textbf{{feature name}} & \\textbf{{data type}} & \\textbf{{description}} \\\\ \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table*}}\n",
    "\\subsection{{Statistical Properties}} {du_statistics_summary}\n",
    "\\subsection{{Data Quality}} {du_quality_summary}\n",
    "\\subsection{{visual exploration}}\n",
    "{du_viz_summary}\n",
    "\\begin{{figure}}[h]\n",
    "    \\centering\n",
    "    \\includegraphics[width=0.8\\linewidth]{{{du_viz_path}}}\n",
    "    \\caption{{visual analysis of obesity factors.}}\n",
    "    \\label{{fig:viz_2d}}\n",
    "\\end{{figure}}\n",
    "\\subsection{{Ethical Sensitivity}} {du_ethics_summary}\n",
    "\n",
    "%% --- 3. DATA PREPARATION ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Applied Actions}} {dp_summary}\n",
    "\\subsection{{Rejected Steps}} {dp_rejected}\n",
    "\\subsection{{Derived Attributes}} {dp_derived}\n",
    "\n",
    "%% --- 4. MODELING ---\n",
    "\\section{{Modeling}}\n",
    "\\subsection{{Algorithm Selection}}\n",
    "{mod_algo_text}\n",
    "\\subsection{{Hyperparameter Identification and Tuning}}\n",
    "{mod_hp_text}\n",
    "\\textit{{Note: tuning results visualized in the attached plots.}}\n",
    "\\subsection{{Data Splitting Strategy}}\n",
    "{mod_split_text}\n",
    "\\subsection{{Final Model Retraining}}\n",
    "{mod_retrain_text}\n",
    "\n",
    "%% --- 5. EVALUATION ---\n",
    "\\section{{Evaluation}}\n",
    "\\subsection{{Final Test Performance}}\n",
    "{eval_main_text}\n",
    "\\textbf{{Resulting Test Accuracy:}} {eval_perf_val}\n",
    "\\subsection{{Baseline and State-of-the-Art Comparison}}\n",
    "{eval_baseline_text}\n",
    "\\subsection{{Detailed Performance Analysis}}\n",
    "{eval_detailed_text}\\\n",
    "\\subsection{{Success Criteria Assessment}}\n",
    "{eval_success_text}\\\n",
    "\\subsection{{Bias and Fairness Analysis}}\n",
    "{eval_bias_text}\\\\\n",
    "\n",
    "\n",
    "%% --- 6. DEPLOYMENT ---\n",
    "\\section{{Deployment}}\n",
    "\\subsection{{Recommendations}}\n",
    "{dep_rec}\n",
    "\\subsection{{Ethical Risks}}\n",
    "{dep_eth}\n",
    "\\subsection{{Monitoring and Maintenance}}\n",
    "{dep_mon}\n",
    "\\subsection{{Reproducibility Reflection}}\n",
    "{dep_repr}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "the project successfully demonstrated the application of the crisp-dm process\n",
    "to classify obesity levels with high accuracy. the provenance logging\n",
    "ensures full transparency of all modeling and evaluation decisions.\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'author_block_latex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# ++++++++++++++++ FINAL UPDATED LATEX TEMPLATE +++++++++++++++++++++++++\u001B[39;00m\n\u001B[32m      3\u001B[39m latex_content = \u001B[33mrf\u001B[39m\u001B[33m\"\"\"\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mdocumentclass[sigconf]\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33macmart\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      4\u001B[39m \n\u001B[32m      5\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mAtBeginDocument\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mprovidecommand\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mBibTeX\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m Bib\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mTeX \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msetcopyright\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33macmlicensed\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mcopyrightyear\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m2025\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[33m\\\u001B[39m\u001B[33macmYear\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m2025\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[33m\\\u001B[39m\u001B[33macmDOI\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mXXXXXXX.XXXXXXX\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     10\u001B[39m \n\u001B[32m     11\u001B[39m \u001B[33m\\\u001B[39m\u001B[33macmConference[BI 2025]\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBusiness Intelligence Final Report\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     12\u001B[39m \n\u001B[32m     13\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mdocument\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     14\u001B[39m \n\u001B[32m     15\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mtitle\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBI2025 Final Report - Group \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgroup_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[33m%% ---Authors: Dynamically added ---\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m \u001B[38;5;132;01m{\u001B[39;00m\u001B[43mauthor_block_latex\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     18\u001B[39m \n\u001B[32m     19\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mabstract\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[33m  this report documents the complete crisp-dm cycle for group \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgroup_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m analyzing obesity levels.\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[33m  it covers business and data understanding, preparation, modeling using random forest,\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[33m  extensive evaluation including bias analysis, and deployment recommendations.\u001B[39m\n\u001B[32m     23\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mabstract\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     24\u001B[39m \n\u001B[32m     25\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mccsdesc[500]\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mcomputing methodologies~machine learning\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     26\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mkeywords\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mcrisp-dm, provenance, random forest, obesity classification, bias analysis\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     27\u001B[39m \n\u001B[32m     28\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mmaketitle\u001B[39m\n\u001B[32m     29\u001B[39m \n\u001B[32m     30\u001B[39m \u001B[33m%% --- 1. BUSINESS UNDERSTANDING ---\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBusiness Understanding\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     32\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Source and Scenario\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbu_data_source\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     33\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBusiness Objectives\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbu_objectives\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBusiness Success Criteria\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbu_success_crit\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     35\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Mining Goals\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbu_mining_goals\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     36\u001B[39m \n\u001B[32m     37\u001B[39m \u001B[33m%% --- 2. DATA UNDERSTANDING ---\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Understanding\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mDataset Overview\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdu_description\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mAttribute Analysis\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     41\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtable*\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m[t]\u001B[39m\n\u001B[32m     42\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mcaption\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mdataset features\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     43\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33msmall\u001B[39m\n\u001B[32m     44\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtabular\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mp\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m0.18\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlinewidth\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33mp\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m0.12\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlinewidth\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33mp\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m0.62\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlinewidth\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtoprule \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mfeature name\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m & \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mdata type\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m & \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mdescription\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\\\u001B[39m\u001B[33m\\\u001B[39m\u001B[33m \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mmidrule\u001B[39m\n\u001B[32m     46\u001B[39m \u001B[33m    \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdu_table_rows\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     47\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mbottomrule\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtabular\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     49\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtable*\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     50\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mStatistical Properties\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdu_statistics_summary\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     51\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Quality\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdu_quality_summary\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     52\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mvisual exploration\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     53\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mdu_viz_summary\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     54\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mfigure\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m[h]\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mcentering\u001B[39m\n\u001B[32m     56\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mincludegraphics[width=0.8\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlinewidth]\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mdu_viz_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     57\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mcaption\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mvisual analysis of obesity factors.\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     58\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlabel\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mfig:viz_2d\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     59\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mfigure\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     60\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mEthical Sensitivity\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdu_ethics_summary\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     61\u001B[39m \n\u001B[32m     62\u001B[39m \u001B[33m%% --- 3. DATA PREPARATION ---\u001B[39m\n\u001B[32m     63\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Preparation\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     64\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mApplied Actions\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdp_summary\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     65\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mRejected Steps\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdp_rejected\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     66\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mDerived Attributes\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdp_derived\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     67\u001B[39m \n\u001B[32m     68\u001B[39m \u001B[33m%% --- 4. MODELING ---\u001B[39m\n\u001B[32m     69\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mModeling\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     70\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mAlgorithm Selection\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mmod_algo_text\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     72\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mHyperparameter Identification and Tuning\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     73\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mmod_hp_text\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     74\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mtextit\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mNote: tuning results visualized in the attached plots.\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     75\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Splitting Strategy\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     76\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mmod_split_text\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     77\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mFinal Model Retraining\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     78\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mmod_retrain_text\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     79\u001B[39m \n\u001B[32m     80\u001B[39m \u001B[33m%% --- 5. EVALUATION ---\u001B[39m\n\u001B[32m     81\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mEvaluation\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     82\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mFinal Test Performance\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     83\u001B[39m \u001B[38;5;132;01m{\u001B[39;00meval_main_text\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     84\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mResulting Test Accuracy:\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00meval_perf_val\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     85\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBias and Fairness Analysis\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     86\u001B[39m \u001B[38;5;132;01m{\u001B[39;00meval_bias_text\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     87\u001B[39m \n\u001B[32m     88\u001B[39m \u001B[33m%% --- 6. DEPLOYMENT ---\u001B[39m\n\u001B[32m     89\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mDeployment\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     90\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mRecommendations\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     91\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mdep_rec\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     92\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mEthical Risks\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     93\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mdep_eth\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     94\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mMonitoring and Maintenance\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     95\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mdep_mon\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     96\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mReproducibility Reflection\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mdep_repr\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     98\u001B[39m \n\u001B[32m     99\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mConclusion\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m    100\u001B[39m \u001B[33mthe project successfully demonstrated the application of the crisp-dm process\u001B[39m\n\u001B[32m    101\u001B[39m \u001B[33mto classify obesity levels with high accuracy. the provenance logging\u001B[39m\n\u001B[32m    102\u001B[39m \u001B[33mensures full transparency of all modeling and evaluation decisions.\u001B[39m\n\u001B[32m    103\u001B[39m \n\u001B[32m    104\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mdocument\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m    105\u001B[39m \u001B[33m\"\"\"\u001B[39m\n",
      "\u001B[31mNameError\u001B[39m: name 'author_block_latex' is not defined"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "b7858f1f4d7bbfa4",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "id": "f727ed5598949e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:19:23.210078Z",
     "start_time": "2026-01-18T16:19:23.172089Z"
    }
   },
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "Describe your Data preparation steps here and include respective graph data.\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'author_block_latex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[54]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m      1\u001B[39m latex_content = \u001B[33mrf\u001B[39m\u001B[33m\"\"\"\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mdocumentclass[sigconf]\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33macmart\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      2\u001B[39m \n\u001B[32m      3\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mAtBeginDocument\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mprovidecommand\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mBibTeX\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m Bib\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mTeX \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msetcopyright\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33macmlicensed\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mcopyrightyear\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m2025\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[33m\\\u001B[39m\u001B[33macmYear\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m2025\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[33m\\\u001B[39m\u001B[33macmDOI\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mXXXXXXX.XXXXXXX\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m      8\u001B[39m \n\u001B[32m      9\u001B[39m \u001B[33m\\\u001B[39m\u001B[33macmConference[BI 2025]\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBusiness Intelligence\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     10\u001B[39m \n\u001B[32m     11\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mdocument\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     12\u001B[39m \n\u001B[32m     13\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mtitle\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBI2025 Experiment Report - Group \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgroup_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[33m%% ---Authors: Dynamically added ---\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m \u001B[38;5;132;01m{\u001B[39;00m\u001B[43mauthor_block_latex\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     16\u001B[39m \n\u001B[32m     17\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mabstract\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[33m  This report documents the machine learning experiment for Group \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgroup_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, following the CRISP-DM process model.\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mabstract\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     20\u001B[39m \n\u001B[32m     21\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mccsdesc[500]\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mComputing methodologies~Machine learning\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mkeywords\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mCRISP-DM, Provenance, Knowledge Graph, Machine Learning\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     23\u001B[39m \n\u001B[32m     24\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mmaketitle\u001B[39m\n\u001B[32m     25\u001B[39m \n\u001B[32m     26\u001B[39m \u001B[33m%% --- 1. Business Understanding ---\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBusiness Understanding\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     28\u001B[39m \n\u001B[32m     29\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Source and Scenario\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mbu_data_source\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     31\u001B[39m \n\u001B[32m     32\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mBusiness Objectives\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     33\u001B[39m \u001B[38;5;132;01m{\u001B[39;00mbu_objectives\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     34\u001B[39m \n\u001B[32m     35\u001B[39m \u001B[33m%% --- 2. Data Understanding ---\u001B[39m\n\u001B[32m     36\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Understanding\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     37\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mDataset Description:\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdu_description\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     38\u001B[39m \n\u001B[32m     39\u001B[39m \u001B[33mThe following features were identified in the dataset:\u001B[39m\n\u001B[32m     40\u001B[39m \n\u001B[32m     41\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtable\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m[h]\u001B[39m\n\u001B[32m     42\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mcaption\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mRaw Data Features\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     43\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlabel\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtab:features\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     44\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtabular\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mlp\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m0.2\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlinewidth\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33mp\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m0.4\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlinewidth\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtoprule\u001B[39m\n\u001B[32m     46\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mFeature Name\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m & \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Type\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m & \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mDescription\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\\\u001B[39m\u001B[33m\\\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mmidrule\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[33m    \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdu_table_rows\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     49\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mbottomrule\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtabular\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     51\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtable\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     52\u001B[39m \n\u001B[32m     53\u001B[39m \u001B[33m%% --- 3. Data Preparation ---\u001B[39m\n\u001B[32m     54\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Preparation\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     55\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mData Cleaning\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     56\u001B[39m \u001B[33mDescribe your Data preparation steps here and include respective graph data.\u001B[39m\n\u001B[32m     57\u001B[39m \n\u001B[32m     58\u001B[39m \n\u001B[32m     59\u001B[39m \u001B[33m%% --- 4. Modeling ---\u001B[39m\n\u001B[32m     60\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mModeling\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     61\u001B[39m \n\u001B[32m     62\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mHyperparameter Configuration\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     63\u001B[39m \u001B[33mThe model was trained using the following hyperparameter settings:\u001B[39m\n\u001B[32m     64\u001B[39m \n\u001B[32m     65\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtable\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m[h]\u001B[39m\n\u001B[32m     66\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mcaption\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mHyperparameter Settings\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     67\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlabel\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtab:hyperparams\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     68\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtabular\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mlp\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33m0.4\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlinewidth\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33ml\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     69\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtoprule\u001B[39m\n\u001B[32m     70\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mParameter\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m & \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mDescription\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m & \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mValue\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\\\u001B[39m\u001B[33m\\\u001B[39m\n\u001B[32m     71\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mmidrule\u001B[39m\n\u001B[32m     72\u001B[39m \u001B[33m    \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhp_table_rows\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     73\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mbottomrule\u001B[39m\n\u001B[32m     74\u001B[39m \u001B[33m  \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtabular\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     75\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mtable\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     76\u001B[39m \n\u001B[32m     77\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msubsection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mTraining Run\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     78\u001B[39m \u001B[33mA training run was executed with the following characteristics:\u001B[39m\n\u001B[32m     79\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mbegin\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mitemize\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     80\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mitem \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mAlgorithm:\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmod_algo\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     81\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mitem \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mStart Time:\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmod_start\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     82\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mitem \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mEnd Time:\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmod_end\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     83\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mitem \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtextbf\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mResult:\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmod_m_lbl\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmod_m_val\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     84\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mitemize\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     85\u001B[39m \n\u001B[32m     86\u001B[39m \u001B[33m%% --- 5. Evaluation ---\u001B[39m\n\u001B[32m     87\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mEvaluation\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     88\u001B[39m \n\u001B[32m     89\u001B[39m \u001B[33m%% --- 6. Deployment ---\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mDeployment\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     91\u001B[39m \n\u001B[32m     92\u001B[39m \u001B[33m\\\u001B[39m\u001B[33msection\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mConclusion\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     93\u001B[39m \n\u001B[32m     94\u001B[39m \u001B[33m\\\u001B[39m\u001B[33mend\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[33mdocument\u001B[39m\u001B[38;5;130;01m}}\u001B[39;00m\n\u001B[32m     95\u001B[39m \u001B[33m\"\"\"\u001B[39m\n",
      "\u001B[31mNameError\u001B[39m: name 'author_block_latex' is not defined"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "a7a01974c59c2074",
   "metadata": {},
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BI2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
